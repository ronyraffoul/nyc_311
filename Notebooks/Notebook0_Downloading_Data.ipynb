{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_TOKEN= \"ulPqb5RthrFBxMJ6hKWEpDtvx\"\n",
    "DATASET_IDENTIFIER = \"erm2-nwe9\"\n",
    "TOTAL_LIMIT = 5000  # Total records to fetch\n",
    "BATCH_SIZE = 1000   # Number of records per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnoise_complaints = fetch_nyc_311_data(\\n    start_date=\"2023-01-01\",\\n    end_date=\"2023-12-31\",\\n    where_clause=\"complaint_type like \\'%Noise%\\'\",\\n    app_token=APP_TOKEN\\n)\\nnoise_complaints.to_csv(\"nyc_noise_complaints_2023.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sodapy import Socrata\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_nyc_311_data(\n",
    "    start_date=None, \n",
    "    end_date=None, \n",
    "    limit=50000, \n",
    "    offset=0, \n",
    "    where_clause=None,\n",
    "    app_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch NYC 311 Service Request data from Socrata API with pagination and date filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_date : str, optional\n",
    "        Start date in format 'YYYY-MM-DD'\n",
    "    end_date : str, optional\n",
    "        End date in format 'YYYY-MM-DD'\n",
    "    limit : int, optional\n",
    "        Number of records to return per request (max 50000)\n",
    "    offset : int, optional\n",
    "        Number of records to skip\n",
    "    where_clause : str, optional\n",
    "        Additional WHERE clause for SoQL query\n",
    "    app_token : str, optional\n",
    "        Socrata App Token for higher rate limits\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the 311 service requests\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Socrata client\n",
    "    client = Socrata(\n",
    "        \"data.cityofnewyork.us\",\n",
    "        app_token=app_token\n",
    "    )\n",
    "    \n",
    "    # Build the SoQL query\n",
    "    query = []\n",
    "    \n",
    "    if start_date and end_date:\n",
    "        query.append(f\"created_date between '{start_date}T00:00:00' and '{end_date}T23:59:59'\")\n",
    "    elif start_date:\n",
    "        query.append(f\"created_date >= '{start_date}T00:00:00'\")\n",
    "    elif end_date:\n",
    "        query.append(f\"created_date <= '{end_date}T23:59:59'\")\n",
    "    \n",
    "    if where_clause:\n",
    "        query.append(f\"({where_clause})\")\n",
    "    \n",
    "    # Combine all query parts with AND\n",
    "    where_query = \" AND \".join(query) if query else None\n",
    "    \n",
    "    # Fetch data with pagination\n",
    "    results = client.get(\n",
    "        \"erm2-nwe9\", \n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "        where=where_query\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    # Close client connection\n",
    "    client.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def download_data_in_chunks(\n",
    "    start_year=2010,\n",
    "    end_year=2025,\n",
    "    chunk_size=3, # months per chunk\n",
    "    records_per_request=50000,\n",
    "    output_format=\"csv\",\n",
    "    app_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download NYC 311 data in chunks by date ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_year : int\n",
    "        Starting year for data download\n",
    "    end_year : int\n",
    "        Ending year for data download\n",
    "    chunk_size : int\n",
    "        Number of months per chunk\n",
    "    records_per_request : int\n",
    "        Number of records per API request (max 50000)\n",
    "    output_format : str\n",
    "        Output file format ('csv' or 'parquet')\n",
    "    app_token : str, optional\n",
    "        Socrata App Token for higher rate limits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create date ranges in chunks of specified months\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    \n",
    "    chunk_number = 1\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        # Calculate end of current chunk\n",
    "        chunk_end = current_date + timedelta(days=30*chunk_size)\n",
    "        if chunk_end > end_date:\n",
    "            chunk_end = end_date\n",
    "            \n",
    "        # Format dates for query\n",
    "        start_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        end_str = chunk_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Downloading Chunk {chunk_number}: {start_str} to {end_str}\")\n",
    "        \n",
    "        # Initialize variables for pagination\n",
    "        offset = 0\n",
    "        has_more_data = True\n",
    "        chunk_df_list = []\n",
    "        \n",
    "        # Download this chunk with pagination\n",
    "        while has_more_data:\n",
    "            try:\n",
    "                print(f\"  Fetching records {offset} to {offset + records_per_request}\")\n",
    "                chunk_data = fetch_nyc_311_data(\n",
    "                    start_date=start_str,\n",
    "                    end_date=end_str,\n",
    "                    limit=records_per_request,\n",
    "                    offset=offset,\n",
    "                    app_token=app_token\n",
    "                )\n",
    "                \n",
    "                # If we got fewer records than the limit, we've reached the end\n",
    "                if len(chunk_data) < records_per_request:\n",
    "                    has_more_data = False\n",
    "                \n",
    "                # Append data to list\n",
    "                if not chunk_data.empty:\n",
    "                    chunk_df_list.append(chunk_data)\n",
    "                    \n",
    "                # Update offset for next page\n",
    "                offset += records_per_request\n",
    "                \n",
    "                # Sleep to avoid hitting rate limits\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading data: {e}\")\n",
    "                # Add exponential backoff retry logic if needed\n",
    "                time.sleep(5)\n",
    "        \n",
    "        # Combine all pages for this chunk\n",
    "        if chunk_df_list:\n",
    "            combined_df = pd.concat(chunk_df_list, ignore_index=True)\n",
    "            \n",
    "            # Save to file\n",
    "            filename = f\"nyc_311_data_{start_str}_to_{end_str}\"\n",
    "            if output_format.lower() == \"csv\":\n",
    "                combined_df.to_csv(f\"{filename}.csv\", index=False)\n",
    "            elif output_format.lower() == \"parquet\":\n",
    "                combined_df.to_parquet(f\"{filename}.parquet\", index=False)\n",
    "            \n",
    "            print(f\"  Saved {len(combined_df)} records to {filename}.{output_format}\")\n",
    "        else:\n",
    "            print(f\"  No data found for date range {start_str} to {end_str}\")\n",
    "        \n",
    "        # Move to next chunk\n",
    "        current_date = chunk_end + timedelta(days=1)\n",
    "        chunk_number += 1\n",
    "\n",
    "\n",
    "# Example with additional filters\n",
    "# Only download noise complaints for a specific date range\n",
    "\"\"\"\n",
    "noise_complaints = fetch_nyc_311_data(\n",
    "    start_date=\"2023-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    where_clause=\"complaint_type like '%Noise%'\",\n",
    "    app_token=APP_TOKEN\n",
    ")\n",
    "noise_complaints.to_csv(\"nyc_noise_complaints_2023.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data in 3-month chunks from 2010 to 2025\n",
    "download_data_in_chunks(\n",
    "    start_year=2024,\n",
    "    end_year=2025,\n",
    "    chunk_size=3,\n",
    "    records_per_request=150000,\n",
    "    output_format=\"csv\",\n",
    "    app_token=APP_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csv_files(input_directory, pattern=\"nyc_311_data_*.csv\", output_file=\"nyc_311_data_combined.csv\"):\n",
    "    \"\"\"\n",
    "    Combines CSV files matching the given pattern in the specified directory\n",
    "    into one CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_directory : str\n",
    "        The directory where the CSV files are located.\n",
    "    pattern : str, optional\n",
    "        The glob pattern to match CSV files. Default is \"nyc_311_data_*.csv\".\n",
    "    output_file : str, optional\n",
    "        The name (and path) of the output combined CSV file.\n",
    "    \"\"\"\n",
    "    # Create full file pattern path\n",
    "    file_pattern = os.path.join(input_directory, pattern)\n",
    "    # List all CSV files matching the pattern\n",
    "    csv_files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found matching the pattern:\", file_pattern)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} files. Combining them...\")\n",
    "    \n",
    "    # List to hold individual DataFrames\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        print(f\"Reading: {file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if data_frames:\n",
    "        # Concatenate all DataFrames into one\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "        # Save the combined DataFrame to CSV\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No data frames were created from the CSV files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory where your CSV files are stored (e.g., current folder)\n",
    "input_dir = \".\"  # Change this if your files are in a different directory\n",
    "output_csv = \"nyc_311_data_combined.csv\"\n",
    "combine_csv_files(input_dir, output_file=output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "\n",
    "def combine_csv_files_polars(\n",
    "    input_directory, \n",
    "    pattern=\"nyc_311_data_*.csv\", \n",
    "    output_file=\"nyc_311_data_combined.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Combines CSV files matching the given pattern in the specified directory into one CSV file using Polars.\n",
    "    It treats \"NA\", \"Unkno\", and \"Unko\" as null values (useful for columns like 'incident_zip') and increases \n",
    "    the schema inference length to reduce type conflicts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_directory : str\n",
    "        The directory where the CSV files are located.\n",
    "    pattern : str, optional\n",
    "        The glob pattern to match CSV files. Default is \"nyc_311_data_*.csv\".\n",
    "    output_file : str, optional\n",
    "        The name (and path) of the output combined CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_pattern = os.path.join(input_directory, pattern)\n",
    "    csv_files = glob.glob(file_pattern)\n",
    "    \n",
    "    # Exclude the combined output file if it exists in the same folder.\n",
    "    csv_files = [f for f in csv_files if os.path.basename(f) != os.path.basename(output_file)]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found matching the pattern:\", file_pattern)\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} files. Combining them...\")\n",
    "\n",
    "    # List to hold individual Polars DataFrames\n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        print(f\"Reading: {file}\")\n",
    "        try:\n",
    "            # Specify null_values, increase infer_schema_length, and ignore errors\n",
    "            df = pl.read_csv(\n",
    "                file, \n",
    "                null_values=[\"NA\", \"Unkno\", \"Unko\"],\n",
    "                infer_schema_length=10000,\n",
    "                ignore_errors=True\n",
    "            )\n",
    "            data_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if data_frames:\n",
    "        # Concatenate all DataFrames into one\n",
    "        combined_df = pl.concat(data_frames)\n",
    "        # Save the combined DataFrame to CSV\n",
    "        combined_df.write_csv(output_file)\n",
    "        print(f\"Combined CSV saved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No data frames were created from the CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the directory where your CSV files are stored (e.g., current folder)\n",
    "input_dir = \".\"  # Change this if your files are in a different directory\n",
    "output_csv = \"nyc_311_data_combined2.csv\"\n",
    "combine_csv_files_polars(input_dir, output_file=output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sodapy import Socrata\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_nyc_311_data(\n",
    "    start_date=None, \n",
    "    end_date=None, \n",
    "    limit=50000, \n",
    "    offset=0, \n",
    "    where_clause=None,\n",
    "    app_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch NYC 311 Service Request data from Socrata API with pagination and date filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_date : str, optional\n",
    "        Start date in format 'YYYY-MM-DD'\n",
    "    end_date : str, optional\n",
    "        End date in format 'YYYY-MM-DD'\n",
    "    limit : int, optional\n",
    "        Number of records to return per request (up to 50,000)\n",
    "    offset : int, optional\n",
    "        Number of records to skip (for pagination)\n",
    "    where_clause : str, optional\n",
    "        Additional WHERE clause for the SoQL query.\n",
    "    app_token : str, optional\n",
    "        Socrata App Token for higher rate limits.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the 311 service requests.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Socrata client\n",
    "    client = Socrata(\"data.cityofnewyork.us\", app_token=app_token)\n",
    "    \n",
    "    # Build the SoQL query parts for the WHERE clause\n",
    "    query_parts = []\n",
    "    if start_date and end_date:\n",
    "        query_parts.append(f\"created_date between '{start_date}T00:00:00' and '{end_date}T23:59:59'\")\n",
    "    elif start_date:\n",
    "        query_parts.append(f\"created_date >= '{start_date}T00:00:00'\")\n",
    "    elif end_date:\n",
    "        query_parts.append(f\"created_date <= '{end_date}T23:59:59'\")\n",
    "    \n",
    "    if where_clause:\n",
    "        query_parts.append(f\"({where_clause})\")\n",
    "    \n",
    "    # Combine query parts with AND if any exist\n",
    "    where_query = \" AND \".join(query_parts) if query_parts else None\n",
    "    \n",
    "    # Add an order parameter to ensure consistent ordering for pagination.\n",
    "    order_by = \"created_date\"\n",
    "    \n",
    "    # Fetch data with pagination using $limit, $offset, and $order\n",
    "    results = client.get(\n",
    "        \"erm2-nwe9\", \n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "        where=where_query,\n",
    "        order=order_by\n",
    "    )\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    # Close the client connection\n",
    "    client.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def download_data_in_chunks(\n",
    "    start_year=2010,\n",
    "    end_year=2025,\n",
    "    chunk_size=3, # months per chunk\n",
    "    records_per_request=50000,\n",
    "    output_format=\"csv\",\n",
    "    app_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download NYC 311 data in chunks (by date ranges) using pagination.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_year : int\n",
    "        Starting year for data download.\n",
    "    end_year : int\n",
    "        Ending year for data download.\n",
    "    chunk_size : int\n",
    "        Number of months per chunk.\n",
    "    records_per_request : int\n",
    "        Number of records per API request (max 50000).\n",
    "    output_format : str\n",
    "        Output file format ('csv' or 'parquet').\n",
    "    app_token : str, optional\n",
    "        Socrata App Token for higher rate limits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create date ranges in chunks of approximately chunk_size months.\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    \n",
    "    chunk_number = 1\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        # Calculate approximate end of current chunk\n",
    "        chunk_end = current_date + timedelta(days=30 * chunk_size)\n",
    "        if chunk_end > end_date:\n",
    "            chunk_end = end_date\n",
    "            \n",
    "        # Format dates for the query\n",
    "        start_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        end_str = chunk_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Downloading Chunk {chunk_number}: {start_str} to {end_str}\")\n",
    "        \n",
    "        # Initialize variables for pagination within the current chunk\n",
    "        offset = 0\n",
    "        has_more_data = True\n",
    "        chunk_df_list = []\n",
    "        \n",
    "        # Download data for this chunk with pagination\n",
    "        while has_more_data:\n",
    "            try:\n",
    "                print(f\"  Fetching records {offset} to {offset + records_per_request - 1}\")\n",
    "                chunk_data = fetch_nyc_311_data(\n",
    "                    start_date=start_str,\n",
    "                    end_date=end_str,\n",
    "                    limit=records_per_request,\n",
    "                    offset=offset,\n",
    "                    app_token=app_token\n",
    "                )\n",
    "                \n",
    "                # If we received fewer records than requested, assume end of data for this chunk\n",
    "                if len(chunk_data) < records_per_request:\n",
    "                    has_more_data = False\n",
    "                \n",
    "                if not chunk_data.empty:\n",
    "                    chunk_df_list.append(chunk_data)\n",
    "                \n",
    "                offset += records_per_request\n",
    "                \n",
    "                # Sleep briefly to avoid hitting rate limits\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading data: {e}\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        # Combine all paginated pages for this chunk\n",
    "        if chunk_df_list:\n",
    "            combined_df = pd.concat(chunk_df_list, ignore_index=True)\n",
    "            \n",
    "            # Save to file in the chosen format\n",
    "            filename = f\"nyc_311_data_{start_str}_to_{end_str}\"\n",
    "            if output_format.lower() == \"csv\":\n",
    "                combined_df.to_csv(f\"{filename}.csv\", index=False)\n",
    "            elif output_format.lower() == \"parquet\":\n",
    "                combined_df.to_parquet(f\"{filename}.parquet\", index=False)\n",
    "            \n",
    "            print(f\"  Saved {len(combined_df)} records to {filename}.{output_format}\")\n",
    "        else:\n",
    "            print(f\"  No data found for date range {start_str} to {end_str}\")\n",
    "        \n",
    "        # Move to the next chunk\n",
    "        current_date = chunk_end + timedelta(days=1)\n",
    "        chunk_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_in_chunks(\n",
    "    start_year=2024,\n",
    "    end_year=2025,\n",
    "    chunk_size=3,\n",
    "    records_per_request=50000,\n",
    "    output_format=\"csv\",\n",
    "    app_token=APP_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sodapy import Socrata\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "\n",
    "# New parameter: increased timeout (in seconds) and max retries\n",
    "TIMEOUT_SECONDS = 120\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def fetch_nyc_311_data(\n",
    "    start_date=None, \n",
    "    end_date=None, \n",
    "    limit=50000, \n",
    "    offset=0, \n",
    "    where_clause=None,\n",
    "    app_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch NYC 311 Service Request data from Socrata API with pagination and date filtering.\n",
    "    \"\"\"\n",
    "    client = Socrata(\"data.cityofnewyork.us\", app_token=app_token, timeout=TIMEOUT_SECONDS)\n",
    "    \n",
    "    query_parts = []\n",
    "    if start_date and end_date:\n",
    "        query_parts.append(f\"created_date between '{start_date}T00:00:00' and '{end_date}T23:59:59'\")\n",
    "    elif start_date:\n",
    "        query_parts.append(f\"created_date >= '{start_date}T00:00:00'\")\n",
    "    elif end_date:\n",
    "        query_parts.append(f\"created_date <= '{end_date}T23:59:59'\")\n",
    "    if where_clause:\n",
    "        query_parts.append(f\"({where_clause})\")\n",
    "    where_query = \" AND \".join(query_parts) if query_parts else None\n",
    "    \n",
    "    order_by = \"created_date\"\n",
    "    \n",
    "    results = client.get(\n",
    "        \"erm2-nwe9\", \n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "        where=where_query,\n",
    "        order=order_by\n",
    "    )\n",
    "    client.close()\n",
    "    \n",
    "    return pd.DataFrame.from_records(results)\n",
    "\n",
    "def fetch_pages_concurrently(start_str, end_str, records_per_request, app_token, max_workers=5):\n",
    "    \"\"\"\n",
    "    Fetch pages concurrently within a date chunk with retries.\n",
    "    \"\"\"\n",
    "    chunk_data_list = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        # Prepare offsets for the next batch of concurrent requests\n",
    "        offsets = [offset + i * records_per_request for i in range(max_workers)]\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_offset = {}\n",
    "            for off in offsets:\n",
    "                # Wrap the fetch call in a retry loop\n",
    "                future = executor.submit(fetch_with_retries, start_str, end_str, records_per_request, off, app_token)\n",
    "                future_to_offset[future] = off\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_offset):\n",
    "                off = future_to_offset[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    results.append((off, data))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error at offset {off}: {e}\")\n",
    "                    results.append((off, pd.DataFrame()))\n",
    "        \n",
    "        # Sort the results by offset\n",
    "        results.sort(key=lambda x: x[0])\n",
    "        stop = False\n",
    "        for off, data in results:\n",
    "            if not data.empty:\n",
    "                chunk_data_list.append(data)\n",
    "            if len(data) < records_per_request:\n",
    "                stop = True\n",
    "        offset += max_workers * records_per_request\n",
    "        if stop:\n",
    "            break\n",
    "        time.sleep(0.5)\n",
    "    return chunk_data_list\n",
    "\n",
    "def fetch_with_retries(start_str, end_str, records_per_request, offset, app_token):\n",
    "    \"\"\"\n",
    "    Attempts to fetch a page with retries if timeouts occur.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f\"  Fetching records {offset} to {offset + records_per_request - 1} (attempt {attempt})\")\n",
    "            df = fetch_nyc_311_data(\n",
    "                start_date=start_str,\n",
    "                end_date=end_str,\n",
    "                limit=records_per_request,\n",
    "                offset=offset,\n",
    "                app_token=app_token\n",
    "            )\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"  Error at offset {offset} on attempt {attempt}: {e}\")\n",
    "            if attempt < MAX_RETRIES:\n",
    "                time.sleep(5 * attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def download_data_in_chunks(\n",
    "    start_year=2010,\n",
    "    end_year=2025,\n",
    "    chunk_size=3,  # months per chunk\n",
    "    records_per_request=50000,\n",
    "    output_format=\"csv\",\n",
    "    app_token=None,\n",
    "    max_workers=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Download NYC 311 data in chunks using concurrent pagination.\n",
    "    \"\"\"\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "    end_date_dt = datetime(end_year, 12, 31)\n",
    "    chunk_number = 1\n",
    "    \n",
    "    while current_date <= end_date_dt:\n",
    "        chunk_end = current_date + timedelta(days=30 * chunk_size)\n",
    "        if chunk_end > end_date_dt:\n",
    "            chunk_end = end_date_dt\n",
    "        \n",
    "        start_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        end_str = chunk_end.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Downloading Chunk {chunk_number}: {start_str} to {end_str}\")\n",
    "        \n",
    "        try:\n",
    "            chunk_df_list = fetch_pages_concurrently(start_str, end_str, records_per_request, app_token, max_workers)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching pages concurrently: {e}\")\n",
    "            chunk_df_list = []\n",
    "        \n",
    "        if chunk_df_list:\n",
    "            combined_df = pd.concat(chunk_df_list, ignore_index=True)\n",
    "            filename = f\"nyc_311_data_{start_str}_to_{end_str}\"\n",
    "            if output_format.lower() == \"csv\":\n",
    "                combined_df.to_csv(f\"{filename}.csv\", index=False)\n",
    "            elif output_format.lower() == \"parquet\":\n",
    "                combined_df.to_parquet(f\"{filename}.parquet\", index=False)\n",
    "            print(f\"  Saved {len(combined_df)} records to {filename}.{output_format}\")\n",
    "        else:\n",
    "            print(f\"  No data found for date range {start_str} to {end_str}\")\n",
    "        \n",
    "        current_date = chunk_end + timedelta(days=1)\n",
    "        chunk_number += 1\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'APP_TOKEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m download_data_in_chunks(\n\u001b[32m      2\u001b[39m         start_year=\u001b[32m2024\u001b[39m,\n\u001b[32m      3\u001b[39m         end_year=\u001b[32m2025\u001b[39m,\n\u001b[32m      4\u001b[39m         chunk_size=\u001b[32m3\u001b[39m,\n\u001b[32m      5\u001b[39m         records_per_request=\u001b[32m50000\u001b[39m,\n\u001b[32m      6\u001b[39m         output_format=\u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         app_token=\u001b[43mAPP_TOKEN\u001b[49m,\n\u001b[32m      8\u001b[39m         max_workers=\u001b[32m5\u001b[39m\n\u001b[32m      9\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'APP_TOKEN' is not defined"
     ]
    }
   ],
   "source": [
    "download_data_in_chunks(\n",
    "        start_year=2024,\n",
    "        end_year=2025,\n",
    "        chunk_size=3,\n",
    "        records_per_request=50000,\n",
    "        output_format=\"csv\",\n",
    "        app_token=APP_TOKEN,\n",
    "        max_workers=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
