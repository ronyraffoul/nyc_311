{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC 311 Service Request Response Time Prediction\n",
    "# Notebook 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import gc  # For garbage collection\n",
    "import pandas as pd  # For compatibility with visualization libs if needed\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data path to sys which is a level above the current directory\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def load_nyc_311_data(file_path):\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    schema_overrides = {\n",
    "        \"unique_key\": pl.Utf8,\n",
    "        \"created_date\": pl.Datetime,\n",
    "        \"closed_date\": pl.Datetime,\n",
    "        \"agency\": pl.Categorical,\n",
    "        \"agency_name\": pl.Categorical,\n",
    "        \"complaint_type\": pl.Categorical,\n",
    "        \"descriptor\": pl.Categorical,\n",
    "        \"status\": pl.Categorical,\n",
    "        \"due_date\": pl.Datetime,\n",
    "        \"resolution_action_updated_date\": pl.Datetime,\n",
    "        \"location_type\": pl.Categorical,\n",
    "        \"incident_zip\": pl.Utf8,\n",
    "        \"incident_address\": pl.Utf8,\n",
    "        \"street_name\": pl.Utf8,\n",
    "        \"cross_street_1\": pl.Utf8,\n",
    "        \"cross_street_2\": pl.Utf8,\n",
    "        \"intersection_street_1\": pl.Utf8,\n",
    "        \"intersection_street_2\": pl.Utf8,\n",
    "        \"address_type\": pl.Categorical,\n",
    "        \"city\": pl.Utf8,\n",
    "        \"landmark\": pl.Utf8,\n",
    "        \"facility_type\": pl.Utf8,\n",
    "        \"community_board\": pl.Categorical,\n",
    "        \"bbl\": pl.Utf8,\n",
    "        \"borough\": pl.Categorical,\n",
    "        \"x_coordinate_state_plane\": pl.Float64,\n",
    "        \"y_coordinate_state_plane\": pl.Float64,\n",
    "        \"open_data_channel_type\": pl.Categorical,\n",
    "        \"latitude\": pl.Float64,\n",
    "        \"longitude\": pl.Float64,\n",
    "        \"location\": pl.Utf8,\n",
    "        \"park_facility_name\": pl.Utf8,\n",
    "        \"park_borough\": pl.Utf8,\n",
    "        \"vehicle_type\": pl.Utf8,\n",
    "        \"taxi_company_borough\": pl.Utf8,\n",
    "        \"taxi_pick_up_location\": pl.Utf8,\n",
    "        \"bridge_highway_name\": pl.Utf8,\n",
    "        \"bridge_highway_direction\": pl.Utf8,\n",
    "        \"road_ramp\": pl.Utf8,\n",
    "        \"bridge_highway_segment\": pl.Utf8\n",
    "    }\n",
    "    \n",
    "    df = pl.scan_csv(\n",
    "        file_path,\n",
    "        schema_overrides=schema_overrides,\n",
    "        null_values=[\"N/A\", \"Unknown\", \"Unkno\", \"\", \"null\"],\n",
    "        infer_schema_length=1000000,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "    \n",
    "    print(\"Data schema:\")\n",
    "    schema = df.collect_schema()\n",
    "    for name, dtype in schema.items():\n",
    "        print(f\"{name}: {dtype}\")\n",
    "    \n",
    "    row_count = df.select(pl.len()).collect()[0, 0]\n",
    "    print(f\"Total rows: {row_count:,}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Data loaded in {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data file - adjust as needed\n",
    "data_file = \"../NYC_311_Data/NYC_311_complete.csv\"\n",
    "\n",
    "# Load data\n",
    "df_lazy = load_nyc_311_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows to verify data loading\n",
    "print(\"Preview of data:\")\n",
    "df_sample = df_lazy.limit(5).collect()\n",
    "display(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Keeping only relevant columns, dropping rows with null closed dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def clean_nyc_311_data(df_lazy):\n",
    "    \"\"\"\n",
    "    Clean NYC 311 data by calculating response time, filtering invalid rows,\n",
    "    and selecting relevant columns for prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_lazy : pl.LazyFrame\n",
    "        Loaded NYC 311 data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pl.LazyFrame\n",
    "        Cleaned data ready for response time prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # First, filter out rows with null 'closed_date'\n",
    "    df_cleaned = df_lazy.filter(\n",
    "        pl.col('closed_date').is_not_null()\n",
    "    )\n",
    "    \n",
    "    # Calculate response time in multiple units\n",
    "    df_cleaned = df_cleaned.with_columns([\n",
    "        (pl.col('closed_date') - pl.col('created_date')).dt.total_seconds().alias('response_time_seconds'),\n",
    "        ((pl.col('closed_date') - pl.col('created_date')).dt.total_seconds() / 60).alias('response_time_minutes'),\n",
    "        ((pl.col('closed_date') - pl.col('created_date')).dt.total_seconds() / 3600).alias('response_time_hours')\n",
    "    ])\n",
    "    \n",
    "    # Filter rows to keep only valid data\n",
    "    df_cleaned = df_cleaned.filter(\n",
    "        pl.col('response_time_hours').is_not_null() &  # Ensures closed_date and created_date are valid\n",
    "        (pl.col('response_time_hours') >= 0) &         # No negative response times\n",
    "        (pl.col('response_time_hours') <= 8760) &      # Max 365 days (365 * 24 hours)\n",
    "        pl.col('agency').is_not_null() &               # Key feature\n",
    "        pl.col('complaint_type').is_not_null() &       # Key feature\n",
    "        pl.col('borough').is_not_null()                # Key feature\n",
    "    )\n",
    "    \n",
    "    # Select only the relevant columns for prediction\n",
    "    columns_to_keep = [\n",
    "        'created_date',           # For temporal features\n",
    "        'agency',                 # Responding agency\n",
    "        'complaint_type',         # Type of complaint\n",
    "        'descriptor',             # Additional detail\n",
    "        'location_type',          # Type of location\n",
    "        'incident_zip',           # Zip code\n",
    "        'borough',                # Borough\n",
    "        'x_coordinate_state_plane',  # Coordinate\n",
    "        'y_coordinate_state_plane',  # Coordinate\n",
    "        'open_data_channel_type', # Submission channel\n",
    "        'latitude',               # Geographic coordinate\n",
    "        'longitude',              # Geographic coordinate\n",
    "        'community_board',        # Local governance area\n",
    "        'response_time_hours'     # Target variable\n",
    "    ]\n",
    "    \n",
    "    df_cleaned = df_cleaned.select(columns_to_keep)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Clean data\n",
    "df_cleaned_lazy = clean_nyc_311_data(df_lazy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_cleaned_lazy.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.write_parquet(\"../NYC_311_Data/outputs/nyc_311_cleaned.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
