{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "schema_overrides = {\n",
    "    \"unique_key\": pl.Int64,\n",
    "    \"created_date\": pl.Datetime,\n",
    "    \"closed_date\": pl.Datetime,\n",
    "    \"agency\": pl.Utf8,\n",
    "    \"complaint_type\": pl.Utf8,\n",
    "    \"descriptor\": pl.Utf8,\n",
    "    \"location_type\": pl.Utf8,\n",
    "    \"incident_zip\": pl.Utf8,\n",
    "    \"borough\": pl.Utf8,\n",
    "    \"open_data_channel_type\": pl.Utf8,\n",
    "    \"latitude\": pl.Float64,\n",
    "    \"longitude\": pl.Float64,\n",
    "    \"community_board\": pl.Utf8,\n",
    "    \"response_time_hours\": pl.Float64,\n",
    "    \"response_time_minutes\": pl.Float64,\n",
    "    \"response_time_seconds\": pl.Float64,\n",
    "\n",
    "}\n",
    "\n",
    "df = pl.read_csv(\n",
    "    \"../NYC_311_Data/outputs/nyc_311_cleaned.csv\",\n",
    "    schema_overrides=schema_overrides,\n",
    "    null_values=[\"NA\", \"N/A\", \"null\", \"\"],  # Treat these as missing\n",
    "    infer_schema_length=1000000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features\n",
    "This section extracts temporal features from created_date to capture time-based patterns:\n",
    "\n",
    "1. day_of_week: Day of the week (1 = Monday, 7 = Sunday).\n",
    "2. hour: Hour of the day (0-23).\n",
    "3. month: Month of the year (1-12).\n",
    "4. week_number: ISO week number of the year\n",
    "4. is_weekend: Binary feature (1 if Saturday or Sunday, 0 otherwise).\n",
    "5. is_rush_hour: Binary feature (1 if time is 7-9 AM or 4-6 PM on weekdays, 0 otherwise).\n",
    "6. is_holiday: Check if a complaint date is a US holiday\n",
    "7. season: Winter, Spring, Summer, Fall\n",
    "8. time_of_day: Night, Morning, Afternoon, Evening "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day of week, Hour and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of week, hour, and month\n",
    "df = df.with_columns([\n",
    "    pl.col('created_date').dt.weekday().alias('day_of_week'),\n",
    "    pl.col('created_date').dt.hour().alias('hour'),\n",
    "    pl.col('created_date').dt.month().alias('month'),\n",
    "    pl.col('created_date').dt.week().alias('week_number')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is weekend or is weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary feature for weekends (Saturday and Sunday)\n",
    "df = df.with_columns((pl.col('day_of_week') >= 6).alias('is_weekend'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rush hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary feature for rush hours (7-9 AM and 4-6 PM on weekdays)\n",
    "df = df.with_columns(\n",
    "    ((pl.col('hour').is_in([7, 8, 9, 16, 17, 18])) & (pl.col('day_of_week') <= 5)).alias('is_rush_hour')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "us_holidays = holidays.US()\n",
    "min_date = df[\"created_date\"].min().date()\n",
    "max_date = df[\"created_date\"].max().date()\n",
    "\n",
    "# Determine the range of years from your dataset\n",
    "years = list(range(min_date.year, max_date.year + 1))\n",
    "us_holidays = holidays.US(years=years)\n",
    "holiday_keys = list(us_holidays.keys())\n",
    "\n",
    "holiday_dates = [date for date in holiday_keys if min_date <= date <= max_date]\n",
    "\n",
    "# Add an 'is_holiday' column to the DataFrame\n",
    "df = df.with_columns(\n",
    "    pl.col(\"created_date\").dt.date().is_in(holiday_dates).alias(\"is_holiday\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"month\").is_in([12, 1, 2])).then(pl.lit(\"Winter\"))\n",
    "    .when(pl.col(\"month\").is_in([3, 4, 5])).then(pl.lit(\"Spring\"))\n",
    "    .when(pl.col(\"month\").is_in([6, 7, 8])).then(pl.lit(\"Summer\"))\n",
    "    .when(pl.col(\"month\").is_in([9, 10, 11])).then(pl.lit(\"Fall\"))\n",
    "    .otherwise(pl.lit(\"Unknown\")).alias(\"season\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.when((pl.col(\"hour\") >= 0) & (pl.col(\"hour\") < 6)).then(pl.lit(\"Night\"))\n",
    "    .when((pl.col(\"hour\") >= 6) & (pl.col(\"hour\") < 12)).then(pl.lit(\"Morning\"))\n",
    "    .when((pl.col(\"hour\") >= 12) & (pl.col(\"hour\") < 18)).then(pl.lit(\"Afternoon\"))\n",
    "    .when((pl.col(\"hour\") >= 18) & (pl.col(\"hour\") < 24)).then(pl.lit(\"Evening\"))\n",
    "    .otherwise(pl.lit(\"Unknown\")).alias(\"time_of_day\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Features\n",
    "This section creates spatial features to capture geographic patterns:\n",
    "\n",
    "1. Borough Dummies: One-hot encoded columns for borough (e.g., 'borough_Manhattan', 'borough_Brooklyn').\n",
    "2. zip_complaint_freq: Frequency of complaints per incident_zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'borough'\n",
    "borough_dummies = df['borough'].to_dummies()\n",
    "df = df.hstack(borough_dummies)\n",
    "del borough_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate complaint frequency per zip code\n",
    "zip_freq = df.group_by('incident_zip').agg(pl.len().alias('zip_complaint_freq'))\n",
    "df = df.join(zip_freq, on='incident_zip', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complaint Characteristics Features\n",
    "This section encodes complaint-related categorical columns using frequency encoding to handle high cardinality:\n",
    "\n",
    "1. agency_freq: Frequency of each agency.\n",
    "2. complaint_type_freq: Frequency of each complaint_type.\n",
    "3. descriptor_freq: Frequency of each descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for 'agency'\n",
    "agency_freq = df.group_by('agency').agg(pl.len().alias('agency_freq'))\n",
    "df = df.join(agency_freq, on='agency', how='left')\n",
    "\n",
    "# Frequency encoding for 'complaint_type'\n",
    "complaint_type_freq = df.group_by('complaint_type').agg(pl.len().alias('complaint_type_freq'))\n",
    "df = df.join(complaint_type_freq, on='complaint_type', how='left')\n",
    "\n",
    "\n",
    "# Frequency encoding for 'descriptor'\n",
    "descriptor_freq = df.group_by('descriptor').agg(pl.len().alias('descriptor_freq'))\n",
    "df = df.join(descriptor_freq, on='descriptor', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Averages\n",
    "\n",
    "1. Daily Complaint Counts (7-day and 30-day): This shows recent trends in complaint volume, useful for workload analysis.\n",
    "2. Response Times (7-day and 30-day): This tracks efficiency trends, helping identify periods of slower or faster resolutions.\n",
    "3. Complaint Counts by Type (7-day and 30-day): This captures trends for specific complaint types, like noise complaints increasing over time.\n",
    "4. Complaint Counts by Borough (7-day and 30-day): This shows borough-specific trends, like more complaints in Manhattan during certain periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"created_date\").dt.date().alias(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily complaint counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = df.group_by(\"date\").agg(pl.len().alias(\"daily_complaints\"))\n",
    "daily_counts = daily_counts.sort(\"date\").with_columns(\n",
    "    pl.col(\"daily_complaints\").rolling_mean(window_size=7).alias(\"7_day_avg_complaints\"),\n",
    "    pl.col(\"daily_complaints\").rolling_mean(window_size=30).alias(\"30_day_avg_complaints\")\n",
    ")\n",
    "df = df.join(daily_counts, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_response = df.group_by(\"date\").agg(pl.col(\"response_time_hours\").mean().alias(\"daily_avg_response\"))\n",
    "daily_response = daily_response.sort(\"date\").with_columns(\n",
    "    pl.col(\"daily_avg_response\").rolling_mean(window_size=7).alias(\"7_day_avg_response\"),\n",
    "    pl.col(\"daily_avg_response\").rolling_mean(window_size=30).alias(\"30_day_avg_response\")\n",
    ")\n",
    "df = df.join(daily_response, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complaint Counts by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_type_counts = df.group_by([\"date\", \"complaint_type\"]).agg(pl.len().alias(\"daily_type_complaints\"))\n",
    "rolling_type_counts = daily_type_counts.sort([\"complaint_type\", \"date\"]).group_by(\"complaint_type\").map_groups(\n",
    "    lambda group: group.with_columns(\n",
    "        pl.col(\"daily_type_complaints\").rolling_mean(window_size=7).alias(\"7_day_avg_type_complaints\"),\n",
    "        pl.col(\"daily_type_complaints\").rolling_mean(window_size=30).alias(\"30_day_avg_type_complaints\")\n",
    "    )\n",
    ")\n",
    "df = df.join(rolling_type_counts, on=[\"date\", \"complaint_type\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complaint Counts by Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_borough_counts = df.group_by([\"date\", \"borough\"]).agg(pl.len().alias(\"daily_borough_complaints\"))\n",
    "rolling_borough_counts = daily_borough_counts.sort([\"borough\", \"date\"]).group_by(\"borough\").map_groups(\n",
    "    lambda group: group.with_columns(\n",
    "        pl.col(\"daily_borough_complaints\").rolling_mean(window_size=7).alias(\"7_day_avg_borough_complaints\"),\n",
    "        pl.col(\"daily_borough_complaints\").rolling_mean(window_size=30).alias(\"30_day_avg_borough_complaints\")\n",
    "    )\n",
    ")\n",
    "df = df.join(rolling_borough_counts, on=[\"date\", \"borough\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the weather data\n",
    "weather_df = pl.read_csv(\"../Weather_data/nyc_weather.csv\", schema_overrides={\n",
    "    'DATE': pl.Date, \n",
    "    'PRCP': pl.Utf8, \n",
    "    'SNOW': pl.Utf8, \n",
    "    'SNWD': pl.Utf8, \n",
    "    'TMAX': pl.Utf8, \n",
    "    'TMIN': pl.Utf8\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define wt_mapping dictionary (based on NOAA GHCN daily documentation)\n",
    "wt_mapping = {\n",
    "    'WT01': 'fog',\n",
    "    'WT02': 'heavy_fog',\n",
    "    'WT03': 'thunder',\n",
    "    'WT04': 'ice_pellets',\n",
    "    'WT05': 'hail',\n",
    "    'WT06': 'glaze',\n",
    "    'WT07': 'dust',\n",
    "    'WT08': 'smoke_haze',\n",
    "    'WT09': 'blowing_snow',\n",
    "    'WT10': 'tornado',\n",
    "    'WT11': 'high_winds',\n",
    "    'WT12': 'blowing_spray',\n",
    "    'WT13': 'mist',\n",
    "    'WT14': 'drizzle',\n",
    "    'WT15': 'freezing_drizzle',\n",
    "    'WT16': 'rain',\n",
    "    'WT17': 'freezing_rain',\n",
    "    'WT18': 'snow',\n",
    "    'WT19': 'unknown_precip',\n",
    "    'WT21': 'ground_fog',\n",
    "    'WT22': 'ice_fog'\n",
    "}\n",
    "\n",
    "# Step 3: Identify WT columns in the dataset\n",
    "wt_cols = [col for col in weather_df.columns if col.startswith('WT') and not col.endswith('_ATTRIBUTES')]\n",
    "\n",
    "# Step 4: Process the weather data\n",
    "weather_df = weather_df.with_columns([\n",
    "    # Handle trace values (replace \"T\" with 0.01) and cast to numeric\n",
    "    pl.col('PRCP').str.replace(\"T\", \"0.01\").cast(pl.Float64).alias('precipitation'),\n",
    "    pl.col('SNOW').str.replace(\"T\", \"0.01\").cast(pl.Float64).alias('snowfall'),\n",
    "    pl.col('SNWD').str.replace(\"T\", \"0.01\").cast(pl.Float64).alias('snow_depth'),\n",
    "    pl.col('TMAX').cast(pl.Float64).alias('max_temperature'),\n",
    "    pl.col('TMIN').cast(pl.Float64).alias('min_temperature'),\n",
    "    # Process WT columns: convert to 0/1 indicators after removing whitespace\n",
    "    *[pl.col(col).str.strip_chars().eq(\"1\").cast(pl.Int8).fill_null(0).alias(wt_mapping.get(col, col.lower())) \n",
    "      for col in wt_cols]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Drop original WT columns and unnecessary columns\n",
    "columns_to_drop = [col for col in weather_df.columns if col.startswith('WT')]\n",
    "weather_df = weather_df.drop(columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Select only the relevant columns\n",
    "weather_df = weather_df.select([\n",
    "    'DATE', 'precipitation', 'snowfall', 'snow_depth', 'max_temperature', 'min_temperature'] + \n",
    "    [wt_mapping[col] for col in wt_cols if col in wt_mapping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Merge with your main DataFrame\n",
    "# Assuming your main DataFrame is called 'df' and has a 'created_date' column\n",
    "df = df.with_columns(pl.col('created_date').dt.date().alias('date'))  # Extract date from created_date\n",
    "df = df.join(weather_df, left_on='date', right_on='DATE', how='left')   # Left join to keep all rows in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operational Workload feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Agency backlog: Number of open requests per agency at time of filing \n",
    "#### Create daily counts of opened and closed requests per agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_agency_tracker = df.select([\n",
    "    pl.col(\"agency\"),\n",
    "    pl.col(\"created_date\").dt.date().alias(\"date\"),\n",
    "    pl.lit(1).alias(\"opened\"),\n",
    "    pl.lit(0).alias(\"closed\")\n",
    "]).vstack(\n",
    "    df.select([\n",
    "        pl.col(\"agency\"),\n",
    "        pl.col(\"closed_date\").dt.date().alias(\"date\"),\n",
    "        pl.lit(0).alias(\"opened\"),\n",
    "        pl.lit(1).alias(\"closed\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by agency and date, sum the opened and closed counts\n",
    "daily_agency_tracker = daily_agency_tracker.group_by([\"agency\", \"date\"]).agg([\n",
    "    pl.sum(\"opened\").alias(\"opened\"),\n",
    "    pl.sum(\"closed\").alias(\"closed\")\n",
    "])\n",
    "\n",
    "# Sort by agency and date\n",
    "daily_agency_tracker = daily_agency_tracker.sort([\"agency\", \"date\"])\n",
    "\n",
    "# Calculate running totals (backlog) for each agency\n",
    "daily_agency_tracker = daily_agency_tracker.group_by(\"agency\").map_groups(\n",
    "    lambda group: group.with_columns([\n",
    "        (pl.col(\"opened\").cum_sum() - pl.col(\"closed\").cum_sum()).alias(\"agency_backlog\")\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join backlog to main dataframe\n",
    "df = df.with_columns(pl.col(\"created_date\").dt.date().alias(\"request_date\"))\n",
    "df = df.join(\n",
    "    daily_agency_tracker.select([\"agency\", \"date\", \"agency_backlog\"]),\n",
    "    left_on=[\"agency\", \"request_date\"],\n",
    "    right_on=[\"agency\", \"date\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Concurrent requests: Number of similar requests filed in the same timeframe \n",
    "#### We'll create both daily and hourly counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronyr\\AppData\\Local\\Temp\\ipykernel_27240\\1099281293.py:8: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  pl.count().alias(\"daily_concurrent_count\")\n",
      "C:\\Users\\ronyr\\AppData\\Local\\Temp\\ipykernel_27240\\1099281293.py:13: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  pl.count().alias(\"hourly_concurrent_count\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"created_date\").dt.date().alias(\"created_date_only\"),\n",
    "    pl.col(\"created_date\").dt.hour().alias(\"created_hour\")\n",
    "])\n",
    "\n",
    "# Daily concurrent complaints of the same type\n",
    "daily_concurrent = df.group_by([\"complaint_type\", \"created_date_only\"]).agg([\n",
    "    pl.len().alias(\"daily_concurrent_count\")\n",
    "])\n",
    "\n",
    "# Hourly concurrent complaints of the same type\n",
    "hourly_concurrent = df.group_by([\"complaint_type\", \"created_date_only\", \"created_hour\"]).agg([\n",
    "    pl.len().alias(\"hourly_concurrent_count\")\n",
    "])\n",
    "\n",
    "# Join both back to the main dataframe\n",
    "df = df.join(daily_concurrent, on=[\"complaint_type\", \"created_date_only\"], how=\"left\")\n",
    "df = df.join(hourly_concurrent, on=[\"complaint_type\", \"created_date_only\", \"created_hour\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Previous day completion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronyr\\AppData\\Local\\Temp\\ipykernel_27240\\1547394833.py:16: DeprecationWarning: Use of `how='outer'` should be replaced with `how='full'`.\n",
      "  daily_stats = daily_opening.join(daily_completion, on=\"date\", how=\"outer\").fill_null(0)\n"
     ]
    }
   ],
   "source": [
    "# Calculate daily completion rate\n",
    "daily_completion = df.filter(pl.col(\"closed_date\").is_not_null()).group_by(\n",
    "    pl.col(\"closed_date\").dt.date().alias(\"date\")\n",
    ").agg([\n",
    "    pl.len().alias(\"closed_count\")\n",
    "])\n",
    "\n",
    "# Get daily opening count\n",
    "daily_opening = df.group_by(\n",
    "    pl.col(\"created_date\").dt.date().alias(\"date\")\n",
    ").agg([\n",
    "    pl.len().alias(\"opened_count\")\n",
    "])\n",
    "\n",
    "# Join and calculate rate\n",
    "daily_stats = daily_opening.join(daily_completion, on=\"date\", how=\"full\").fill_null(0)\n",
    "daily_stats = daily_stats.sort(\"date\").with_columns([\n",
    "    (pl.col(\"closed_count\") / pl.col(\"opened_count\").shift(1)).fill_null(0).alias(\"prev_day_completion_rate\")\n",
    "])\n",
    "\n",
    "# Join back to main dataframe\n",
    "df = df.join(\n",
    "    daily_stats.select([\"date\", \"prev_day_completion_rate\"]), \n",
    "    left_on=\"created_date_only\", \n",
    "    right_on=\"date\", \n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Spatial clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in df.columns for col in [\"latitude\", \"longitude\"]):\n",
    "    # Create grid cells for spatial binning (approximately 500m grid at NYC latitude)\n",
    "    grid_size = 0.005  # ~500m at NYC latitude\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"latitude\").is_not_null() & pl.col(\"longitude\").is_not_null())\n",
    "        .then((pl.col(\"latitude\") / grid_size).floor() * grid_size)  # Use .floor() here\n",
    "        .otherwise(None)\n",
    "        .alias(\"lat_grid\"),\n",
    "\n",
    "        pl.when(pl.col(\"latitude\").is_not_null() & pl.col(\"longitude\").is_not_null())\n",
    "        .then((pl.col(\"longitude\") / grid_size).floor() * grid_size)  # Use .floor() here\n",
    "        .otherwise(None)\n",
    "        .alias(\"lon_grid\")\n",
    "    ])\n",
    "\n",
    "    # Count complaints in each grid cell\n",
    "    grid_density = df.filter(\n",
    "        pl.col(\"lat_grid\").is_not_null() & pl.col(\"lon_grid\").is_not_null()\n",
    "    ).group_by([\"lat_grid\", \"lon_grid\"]).agg([\n",
    "        pl.len().alias(\"grid_complaint_density\")\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Join back to main dataframe\n",
    "    df = df.join(grid_density, on=[\"lat_grid\", \"lon_grid\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Public safety indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety-related complaint types (based on the list analysis)\n",
    "safety_related_types = [\n",
    "    \"Safety\", \"Illegal Fireworks\", \"Unstable Building\", \"Facade Insp Safety Pgm\", \n",
    "    \"Facades\", \"Structural\", \"Forensic Engineering\", \"Asbestos\", \"Hazardous Materials\", \n",
    "    \"Lead\", \"Oil Or Gas Spill\", \"Radioactive Material\", \"Emergency Response Team (Ert)\", \n",
    "    \"Scaffold Safety\", \"Dep Highway Condition\", \"Air Quality\", \"Indoor Air Quality\", \n",
    "    \"X-Ray Machine/Equipment\", \"Cooling Tower\", \"Drinking Water\", \"Water Quality\", \n",
    "    \"Building Drinking Water Tank\", \"Indoor Sewage\", \"Food Poisoning\", \"Highway Sign - Dangling\", \n",
    "    \"Street Sign - Dangling\", \"Building Condition\", \"Face Covering Violation\", \n",
    "    \"Vaccine Mandate Non-Compliance\", \"Private School Vaccine Mandate Non-Compliance\",\n",
    "    \"Traffic Signal Condition\", \"Elevator\", \"Plumbing\", \"Sewer\", \"Water Leak\", \n",
    "    \"Water System\", \"Sewer Maintenance\", \"Electric\", \"Electrical\", \"Flooring/Stairs\", \n",
    "    \"Door/Window\", \"Construction Safety Enforcement\"\n",
    "]\n",
    "\n",
    "# Convert to lowercase for case-insensitive matching\n",
    "safety_related_types_lower = [complaint_type.lower() for complaint_type in safety_related_types]\n",
    "\n",
    "# Create safety indicator\n",
    "df = df.with_columns([\n",
    "    pl.col(\"complaint_type\").str.to_lowercase().is_in(safety_related_types_lower).alias(\"is_safety_related\")\n",
    "])\n",
    "\n",
    "# Create a weighted safety score (1-5) for different categories of safety concerns\n",
    "high_priority_safety = [\"Unstable Building\", \"Emergency Response Team (Ert)\", \n",
    "                       \"Hazardous Materials\", \"Oil Or Gas Spill\", \"Radioactive Material\"]\n",
    "high_priority_lower = [item.lower() for item in high_priority_safety]\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"complaint_type\").str.to_lowercase().is_in(high_priority_lower))\n",
    "    .then(5)  # Highest priority\n",
    "    .when(pl.col(\"is_safety_related\"))\n",
    "    .then(3)  # Medium priority\n",
    "    .otherwise(1)  # Standard priority\n",
    "    .alias(\"safety_priority_score\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  6. Complaint priority score based on historical agency response patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate average response time per complaint type\n",
    "complaint_priority = df.group_by(\"complaint_type\").agg([\n",
    "    pl.mean(\"response_time_hours\").alias(\"avg_response_time\"),\n",
    "    pl.std(\"response_time_hours\").alias(\"std_response_time\"),\n",
    "    pl.len().alias(\"complaint_count\")\n",
    "])\n",
    "\n",
    "# Calculate priority score (inverse of response time - faster response = higher priority)\n",
    "complaint_priority = complaint_priority.with_columns([\n",
    "    (1 / (pl.col(\"avg_response_time\") + 0.001)).alias(\"raw_priority_score\")\n",
    "])\n",
    "\n",
    "# Normalize to 0-1 scale\n",
    "complaint_priority = complaint_priority.with_columns([\n",
    "    (pl.col(\"raw_priority_score\") / pl.col(\"raw_priority_score\").max()).alias(\"priority_score\")\n",
    "])\n",
    "\n",
    "# Join back to main dataframe\n",
    "df = df.join(\n",
    "    complaint_priority.select([\"complaint_type\", \"priority_score\"]),\n",
    "    on=\"complaint_type\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Request escalation indicator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify complaints that typically take much longer than average to resolve\n",
    "escalation_threshold = df.group_by(\"complaint_type\").agg([\n",
    "    pl.mean(\"response_time_hours\").alias(\"mean_response\"),\n",
    "    pl.std(\"response_time_hours\").alias(\"std_response\")\n",
    "]).with_columns([\n",
    "    (pl.col(\"mean_response\") + 2 * pl.col(\"std_response\")).alias(\"escalation_threshold\")\n",
    "])\n",
    "\n",
    "# Join back to main dataframe\n",
    "df = df.join(\n",
    "    escalation_threshold.select([\"complaint_type\", \"escalation_threshold\"]),\n",
    "    on=\"complaint_type\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Mark complaints that exceed threshold\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"response_time_hours\") > pl.col(\"escalation_threshold\")).alias(\"is_likely_escalated\")\n",
    "])\n",
    "\n",
    "# Calculate historical escalation rate by complaint type\n",
    "escalation_rate = df.group_by(\"complaint_type\").agg([\n",
    "    pl.mean(\"is_likely_escalated\").cast(pl.Float32).alias(\"historical_escalation_rate\")\n",
    "])\n",
    "\n",
    "# Join back to main dataframe\n",
    "df = df.join(\n",
    "    escalation_rate,\n",
    "    on=\"complaint_type\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to end of business day (hours remaining until 5pm)\n",
    "df = df.with_columns([\n",
    "    pl.when(\n",
    "        (pl.col(\"hour\") < 17) & (pl.col(\"day_of_week\") < 5)  # Before 5pm on weekdays\n",
    "    ).then(\n",
    "        17 - pl.col(\"hour\")\n",
    "    ).otherwise(\n",
    "        # If after 5pm or weekend, set to 0\n",
    "        pl.lit(0)\n",
    "    ).alias(\"hours_to_end_of_business\")\n",
    "])\n",
    "\n",
    "# Days until weekend\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"day_of_week\") < 5)  # Weekday (0-4 = Mon-Fri)\n",
    "    .then(5 - pl.col(\"day_of_week\"))  # Days until Saturday\n",
    "    .otherwise(pl.lit(0))  # Already weekend\n",
    "    .alias(\"days_to_weekend\")\n",
    "])\n",
    "\n",
    "# Days until next holiday\n",
    "# This is more complex - need to calculate for each date\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get all dates in dataframe\n",
    "unique_dates = df.select(pl.col(\"created_date\").dt.date()).unique().to_series()\n",
    "unique_dates_list = unique_dates.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate days to next holiday for each date\n",
    "days_to_holiday_map = {}\n",
    "for date in unique_dates_list:\n",
    "    # Find next holiday\n",
    "    next_holiday = None\n",
    "    days_diff = float('inf')\n",
    "    \n",
    "    for holiday in holiday_dates:\n",
    "        if holiday > date:\n",
    "            curr_days = (holiday - date).days\n",
    "            if curr_days < days_diff:\n",
    "                days_diff = curr_days\n",
    "                next_holiday = holiday\n",
    "    \n",
    "    days_to_holiday_map[date] = days_diff if days_diff != float('inf') else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create mapping dataframe\n",
    "holiday_mapping = pl.DataFrame({\n",
    "    \"date\": list(days_to_holiday_map.keys()),\n",
    "    \"days_to_next_holiday\": list(days_to_holiday_map.values())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join to main dataframe\n",
    "df = df.join(\n",
    "    holiday_mapping,\n",
    "    left_on=pl.col(\"created_date\").dt.date(),\n",
    "    right_on=\"date\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Interaction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather severity x complaint type interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a weather severity index\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"precipitation\") * 5 + \n",
    "     pl.col(\"snowfall\") * 3 + \n",
    "     pl.col(\"high_winds\").cast(pl.Float64) * 4 +\n",
    "     pl.col(\"thunder\").cast(pl.Float64) * 3 +\n",
    "     ((pl.col(\"max_temperature\") > 90).cast(pl.Float64) * 3) +\n",
    "     ((pl.col(\"min_temperature\") < 20).cast(pl.Float64) * 3)\n",
    "    ).alias(\"weather_severity_index\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create interaction between weather severity and complaint types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First group complaint types into categories that might be weather-sensitive\n",
    "weather_sensitive_complaints = [\n",
    "    \"Street Condition\", \"Flooding\", \"Water System\", \"Sewer\", \"Snow\", \n",
    "    \"Water Leak\", \"Street Light Condition\", \"Heat/Hot Water\"\n",
    "]\n",
    "weather_sensitive_complaints_lower = [c.lower() for c in weather_sensitive_complaints]\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"complaint_type\").str.to_lowercase().is_in(weather_sensitive_complaints_lower).alias(\"is_weather_sensitive\")\n",
    "])\n",
    "\n",
    "# Create interaction feature\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"weather_severity_index\") * pl.col(\"is_weather_sensitive\").cast(pl.Float64)).alias(\"weather_complaint_interaction\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekend x agency interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group agencies by those that likely operate differently on weekends\n",
    "emergency_agencies = [\"FDNY\", \"NYPD\", \"DOHMH\", \"DEP\"]\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"is_weekend\") & pl.col(\"agency\").is_in(emergency_agencies)).alias(\"weekend_emergency_agency\"),\n",
    "    (pl.col(\"is_weekend\") & ~pl.col(\"agency\").is_in(emergency_agencies)).alias(\"weekend_non_emergency_agency\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonal agency workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "for season in seasons:\n",
    "    season_lower = season.lower()\n",
    "    # Create indicator for each season-agency combination\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"season\") == season) & (pl.col(\"agency\") == ag)).alias(f\"{season_lower}_{ag}\")\n",
    "        for ag in df[\"agency\"].unique().to_list()[:10]  # Limit to top 10 agencies to avoid explosion of features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 period indicators\n",
    "df = df.with_columns([\n",
    "    # Pre-COVID (before March 1, 2020)\n",
    "    (pl.col(\"created_date\").dt.date() < pl.date(2020, 3, 1)).alias(\"pre_covid\"),\n",
    "    \n",
    "    # Initial lockdown phase (March-June 2020)\n",
    "    ((pl.col(\"created_date\").dt.date() >= pl.date(2020, 3, 1)) & \n",
    "     (pl.col(\"created_date\").dt.date() < pl.date(2020, 7, 1))).alias(\"covid_lockdown\"),\n",
    "    \n",
    "    # Phased reopening (July-Nov 2020)\n",
    "    ((pl.col(\"created_date\").dt.date() >= pl.date(2020, 7, 1)) & \n",
    "     (pl.col(\"created_date\").dt.date() < pl.date(2020, 12, 1))).alias(\"covid_reopening\"),\n",
    "    \n",
    "    # Winter surge (Dec 2020-Feb 2021)\n",
    "    ((pl.col(\"created_date\").dt.date() >= pl.date(2020, 12, 1)) & \n",
    "     (pl.col(\"created_date\").dt.date() < pl.date(2021, 3, 1))).alias(\"covid_winter_surge\"),\n",
    "    \n",
    "    # Delta variant period (Jun-Nov 2021)\n",
    "    ((pl.col(\"created_date\").dt.date() >= pl.date(2021, 6, 1)) & \n",
    "     (pl.col(\"created_date\").dt.date() < pl.date(2021, 12, 1))).alias(\"covid_delta\"),\n",
    "    \n",
    "    # Omicron variant period (Dec 2021-Feb 2022)\n",
    "    ((pl.col(\"created_date\").dt.date() >= pl.date(2021, 12, 1)) & \n",
    "     (pl.col(\"created_date\").dt.date() < pl.date(2022, 3, 1))).alias(\"covid_omicron\"),\n",
    "    \n",
    "    # Post-major restrictions (after March 2022)\n",
    "    (pl.col(\"created_date\").dt.date() >= pl.date(2022, 3, 1)).alias(\"post_major_covid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC School Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate NYC school calendar indicators\n",
    "df = df.with_columns([\n",
    "    # Summer break (roughly late June through early September)\n",
    "    ((pl.col(\"month\").is_in([7, 8])) | \n",
    "     ((pl.col(\"month\") == 6) & (pl.col(\"created_date\").dt.day() >= 25)) |\n",
    "     ((pl.col(\"month\") == 9) & (pl.col(\"created_date\").dt.day() <= 5))).alias(\"school_summer_break\"),\n",
    "    \n",
    "    # Winter break (roughly Dec 24-Jan 2)\n",
    "    (((pl.col(\"month\") == 12) & (pl.col(\"created_date\").dt.day() >= 24)) | \n",
    "     ((pl.col(\"month\") == 1) & (pl.col(\"created_date\").dt.day() <= 2))).alias(\"school_winter_break\"),\n",
    "    \n",
    "    # Spring break (approximately first/second week of April)\n",
    "    ((pl.col(\"month\") == 4) & (pl.col(\"created_date\").dt.day().is_between(1, 10))).alias(\"school_spring_break\"),\n",
    "])\n",
    "\n",
    "# Create a general \"school_in_session\" indicator\n",
    "df = df.with_columns([\n",
    "    (~(pl.col(\"school_summer_break\") | pl.col(\"school_winter_break\") | \n",
    "       pl.col(\"school_spring_break\") | pl.col(\"is_holiday\")) & \n",
    "     (pl.col(\"day_of_week\") < 5)).alias(\"school_in_session\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC Major Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major NYC events indicators\n",
    "df = df.with_columns([\n",
    "    # NYC Marathon (first Sunday in November)\n",
    "    ((pl.col(\"month\") == 11) & \n",
    "     (pl.col(\"day_of_week\") == 6) & \n",
    "     (pl.col(\"created_date\").dt.day() <= 7)).alias(\"nyc_marathon_day\"),\n",
    "    \n",
    "    # Thanksgiving Parade (Thanksgiving Day)\n",
    "    ((pl.col(\"month\") == 11) & \n",
    "     (pl.col(\"day_of_week\") == 3) & \n",
    "     (pl.col(\"created_date\").dt.day() >= 22) & \n",
    "     (pl.col(\"created_date\").dt.day() <= 28)).alias(\"thanksgiving_parade\"),\n",
    "    \n",
    "    # New Year's Eve/Day \n",
    "    (((pl.col(\"month\") == 12) & (pl.col(\"created_date\").dt.day() == 31)) | \n",
    "     ((pl.col(\"month\") == 1) & (pl.col(\"created_date\").dt.day() == 1))).alias(\"new_years\"),\n",
    "    \n",
    "    # St. Patrick's Day Parade (March 17 or nearest weekend)\n",
    "    ((pl.col(\"month\") == 3) & \n",
    "     (pl.col(\"created_date\").dt.day().is_between(15, 19))).alias(\"st_patricks_parade\"),\n",
    "    \n",
    "    # Pride March (last Sunday in June)\n",
    "    ((pl.col(\"month\") == 6) & \n",
    "     (pl.col(\"day_of_week\") == 6) & \n",
    "     (pl.col(\"created_date\").dt.day() >= 24)).alias(\"pride_march\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather and Complaint type interactive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weather severity indicators\n",
    "df = df.with_columns([\n",
    "    # Heavy rain day (precipitation > 1 inch)\n",
    "    (pl.col(\"precipitation\") > 1.0).alias(\"heavy_rain_day\"),\n",
    "    \n",
    "    # Snow day\n",
    "    (pl.col(\"snowfall\") > 0.5).alias(\"snow_day\"),\n",
    "    \n",
    "    # Extreme heat (max temp > 90°F)\n",
    "    (pl.col(\"max_temperature\") > 90).alias(\"extreme_heat_day\"),\n",
    "    \n",
    "    # Extreme cold (min temp < 20°F)\n",
    "    (pl.col(\"min_temperature\") < 20).alias(\"extreme_cold_day\")\n",
    "])\n",
    "\n",
    "# Map complaint types to categories that would be affected by different weather\n",
    "# Heat-related complaints\n",
    "df = df.with_columns([\n",
    "    pl.col(\"complaint_type\").str.to_lowercase().is_in([\n",
    "        \"heat/hot water\", \"residential heat\", \"cooling tower\"\n",
    "    ]).alias(\"is_heat_related_complaint\")\n",
    "])\n",
    "\n",
    "# Water/flood-related complaints\n",
    "df = df.with_columns([\n",
    "    pl.col(\"complaint_type\").str.to_lowercase().is_in([\n",
    "        \"sewer\", \"water system\", \"water leak\", \"flooding\", \"standing water\", \"plumbing\"\n",
    "    ]).alias(\"is_water_related_complaint\")\n",
    "])\n",
    "\n",
    "# Street condition complaints\n",
    "df = df.with_columns([\n",
    "    pl.col(\"complaint_type\").str.to_lowercase().is_in([\n",
    "        \"street condition\", \"street light condition\", \"sidewalk condition\", \n",
    "        \"highway condition\", \"traffic signal condition\"\n",
    "    ]).alias(\"is_street_related_complaint\")\n",
    "])\n",
    "\n",
    "# Create interaction features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"heavy_rain_day\") & pl.col(\"is_water_related_complaint\")).alias(\"rain_water_interaction\"),\n",
    "    (pl.col(\"snow_day\") & pl.col(\"is_street_related_complaint\")).alias(\"snow_street_interaction\"),\n",
    "    (pl.col(\"extreme_heat_day\") & pl.col(\"is_heat_related_complaint\")).alias(\"heat_complaint_interaction\"),\n",
    "    (pl.col(\"extreme_cold_day\") & pl.col(\"is_heat_related_complaint\")).alias(\"cold_heat_complaint_interaction\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weekend/holiday and complaint type interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns([\n",
    "    (pl.col(\"is_weekend\") & pl.col(\"complaint_type\").str.to_lowercase().is_in([\"noise - residential\", \"noise - street/sidewalk\", \"noise - commercial\"])).alias(\"weekend_noise_complaint\"),\n",
    "    \n",
    "    (pl.col(\"is_holiday\") & pl.col(\"complaint_type\").str.to_lowercase().is_in([\"noise - residential\", \"noise - street/sidewalk\", \"illegal parking\", \"blocked driveway\"])).alias(\"holiday_noise_parking_complaint\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe\n",
    "df.write_csv(\"../NYC_311_Data/outputs/nyc_311_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create borough binary features (one-hot encoding)\n",
    "boroughs = ['MANHATTAN', 'BROOKLYN', 'QUEENS', 'BRONX', 'STATEN ISLAND']\n",
    "for borough in boroughs:\n",
    "    df = df.with_columns([\n",
    "        (pl.col('borough') == borough).cast(pl.Int8).alias(f'borough_{borough.lower().replace(\" \", \"_\")}')\n",
    "    ])\n",
    "\n",
    "# Calculate distances from borough centers (approximation)\n",
    "# These are approximate lat/long coordinates for borough centers\n",
    "borough_coords = {\n",
    "    'MANHATTAN': (40.7831, -73.9712),\n",
    "    'BROOKLYN': (40.6782, -73.9442),\n",
    "    'QUEENS': (40.7282, -73.7949),\n",
    "    'BRONX': (40.8448, -73.8648),\n",
    "    'STATEN ISLAND': (40.5795, -74.1502),\n",
    "    'NYC_CENTER': (40.7128, -74.0060)  # Manhattan downtown\n",
    "}\n",
    "\n",
    "# Create distance features using Haversine formula\n",
    "# First create helper function to calculate distances\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Add distance features for each borough center\n",
    "for borough, (lat, lon) in borough_coords.items():\n",
    "    name = borough.lower().replace(\" \", \"_\")\n",
    "    df = df.with_columns([\n",
    "        haversine_distance(\n",
    "            pl.col('latitude'), \n",
    "            pl.col('longitude'), \n",
    "            lat, \n",
    "            lon\n",
    "        ).alias(f'distance_to_{name}')\n",
    "    ])\n",
    "\n",
    "# ----------------------\n",
    "# Complaint Characteristics\n",
    "# ----------------------\n",
    "\n",
    "# Create frequency features by complaint type and zip code\n",
    "complaint_freq = df.group_by(['complaint_type']).agg(\n",
    "    pl.count().alias('complaint_type_frequency')\n",
    ")\n",
    "\n",
    "zip_freq = df.group_by(['incident_zip']).agg(\n",
    "    pl.count().alias('zip_frequency')\n",
    ")\n",
    "\n",
    "agency_freq = df.group_by(['agency']).agg(\n",
    "    pl.count().alias('agency_frequency')\n",
    ")\n",
    "\n",
    "# Join frequency features back to the main dataframe\n",
    "df = df.join(complaint_freq, on='complaint_type', how='left')\n",
    "df = df.join(zip_freq, on='incident_zip', how='left')\n",
    "df = df.join(agency_freq, on='agency', how='left')\n",
    "\n",
    "# Calculate average response time for each complaint type & agency\n",
    "# This is a form of target encoding\n",
    "avg_response_by_complaint = df.group_by(['complaint_type']).agg(\n",
    "    pl.mean('response_time_hours').alias('avg_response_time_by_complaint')\n",
    ")\n",
    "\n",
    "avg_response_by_agency = df.group_by(['agency']).agg(\n",
    "    pl.mean('response_time_hours').alias('avg_response_time_by_agency')\n",
    ")\n",
    "\n",
    "avg_response_by_agency_complaint = df.group_by(['agency', 'complaint_type']).agg(\n",
    "    pl.mean('response_time_hours').alias('avg_response_time_by_agency_complaint')\n",
    ")\n",
    "\n",
    "# Join average response times back to the main dataframe\n",
    "df = df.join(avg_response_by_complaint, on='complaint_type', how='left')\n",
    "df = df.join(avg_response_by_agency, on='agency', how='left')\n",
    "df = df.join(avg_response_by_agency_complaint, on=['agency', 'complaint_type'], how='left')\n",
    "\n",
    "# ----------------------\n",
    "# Categorical Encoding\n",
    "# ----------------------\n",
    "\n",
    "# Target encoding for high-cardinality categorical features\n",
    "# (We already did a simple version with the average response times above)\n",
    "\n",
    "# For location_type, descriptor, and open_data_channel_type\n",
    "# Smoothed target encoding to prevent overfitting\n",
    "def smoothed_target_encoding(df, col, target, min_samples=100):\n",
    "    \"\"\"Smoothed target encoding for a categorical feature\"\"\"\n",
    "    global_mean = df.select(pl.mean(target)).collect()[0, 0]\n",
    "    \n",
    "    # Calculate the mean target for each category\n",
    "    cat_stats = df.group_by(col).agg([\n",
    "        pl.mean(target).alias(f'mean_{target}'),\n",
    "        pl.count().alias('count')\n",
    "    ])\n",
    "    \n",
    "    # Calculate the smoothed mean\n",
    "    cat_stats = cat_stats.with_columns([\n",
    "        ((pl.col(f'mean_{target}') * pl.col('count') + global_mean * min_samples) / \n",
    "         (pl.col('count') + min_samples)).alias(f'{col}_encoded')\n",
    "    ])\n",
    "    \n",
    "    # Select only the column name and encoded value\n",
    "    cat_stats = cat_stats.select([pl.col(col), pl.col(f'{col}_encoded')])\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    return df.join(cat_stats, on=col, how='left')\n",
    "\n",
    "# Encode high-cardinality categorical features\n",
    "for col in ['location_type', 'descriptor', 'open_data_channel_type']:\n",
    "    df = smoothed_target_encoding(df, col, 'response_time_hours')\n",
    "\n",
    "# ----------------------\n",
    "# Interaction Features\n",
    "# ----------------------\n",
    "\n",
    "# Create interaction features between key variables\n",
    "df = df.with_columns([\n",
    "    (pl.col('complaint_type_frequency') * pl.col('avg_response_time_by_complaint')).alias('complaint_intensity'),\n",
    "    (pl.col('is_weekend') * pl.col('agency_frequency')).alias('weekend_agency_interaction'),\n",
    "    (pl.col('is_holiday') * pl.col('complaint_type_frequency')).alias('holiday_complaint_interaction')\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# Text Features (if needed)\n",
    "# ----------------------\n",
    "\n",
    "# Get complaint type word count\n",
    "df = df.with_columns([\n",
    "    pl.col('complaint_type').str.split(' ').arr.lengths().alias('complaint_type_word_count'),\n",
    "    pl.col('descriptor').str.split(' ').arr.lengths().alias('descriptor_word_count')\n",
    "])\n",
    "\n",
    "# ----------------------\n",
    "# Create Feature Matrix\n",
    "# ----------------------\n",
    "\n",
    "# List of features to keep (excluding the original categorical fields that we've encoded)\n",
    "features_to_keep = [\n",
    "    # Target\n",
    "    'response_time_hours',\n",
    "    \n",
    "    # Temporal features\n",
    "    'year', 'month', 'day', 'hour', 'minute', 'weekday', 'quarter',\n",
    "    'day_of_year', 'week_of_year', 'period_of_day', 'season',\n",
    "    'is_weekend', 'is_business_hours', 'is_rush_hour', 'is_holiday',\n",
    "    \n",
    "    # Spatial features\n",
    "    'zip_region', 'latitude', 'longitude',\n",
    "    'borough_manhattan', 'borough_brooklyn', 'borough_queens', \n",
    "    'borough_bronx', 'borough_staten_island',\n",
    "    'distance_to_manhattan', 'distance_to_brooklyn', 'distance_to_queens',\n",
    "    'distance_to_bronx', 'distance_to_staten_island', 'distance_to_nyc_center',\n",
    "    \n",
    "    # Frequency features\n",
    "    'complaint_type_frequency', 'zip_frequency', 'agency_frequency',\n",
    "    \n",
    "    # Target encoded features\n",
    "    'avg_response_time_by_complaint', 'avg_response_time_by_agency',\n",
    "    'avg_response_time_by_agency_complaint',\n",
    "    'location_type_encoded', 'descriptor_encoded', 'open_data_channel_type_encoded',\n",
    "    \n",
    "    # Interaction features\n",
    "    'complaint_intensity', 'weekend_agency_interaction', 'holiday_complaint_interaction',\n",
    "    \n",
    "    # Text features\n",
    "    'complaint_type_word_count', 'descriptor_word_count'\n",
    "]\n",
    "\n",
    "# Create the final feature matrix\n",
    "feature_matrix = df.select(features_to_keep)\n",
    "\n",
    "# Save the feature matrix as parquet file for efficient storage\n",
    "feature_matrix.collect().write_parquet(\"nyc_311_features.parquet\")\n",
    "\n",
    "print(\"Feature engineering complete! Features saved to nyc_311_features.parquet\")\n",
    "print(f\"Created {len(features_to_keep)} features from the original dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "# Assuming df is your loaded dataframe\n",
    "# df = pl.read_csv(\"nyc_311_data.csv\", columns=columns_to_keep)\n",
    "\n",
    "# ======================================================================\n",
    "# 1. TEMPORAL FEATURES\n",
    "# ======================================================================\n",
    "\n",
    "def create_temporal_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create temporal features from created_date\n",
    "    \"\"\"\n",
    "    # First, ensure created_date is a datetime type\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"created_date\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    ])\n",
    "    \n",
    "    # Extract basic temporal components\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"created_date\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"created_date\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"created_date\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"created_date\").dt.hour().alias(\"hour\"),\n",
    "        pl.col(\"created_date\").dt.minute().alias(\"minute\"),\n",
    "        pl.col(\"created_date\").dt.weekday().alias(\"day_of_week\"),\n",
    "        pl.col(\"created_date\").dt.month().alias(\"month_num\"),\n",
    "        pl.col(\"created_date\").dt.ordinal_day().alias(\"day_of_year\"),\n",
    "        pl.col(\"created_date\").dt.week().alias(\"week_of_year\"),\n",
    "        pl.col(\"created_date\").dt.quarter().alias(\"quarter\")\n",
    "    ])\n",
    "    \n",
    "    # Weekend flag (0 = weekday, 1 = weekend)\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"day_of_week\") >= 5).cast(pl.Int8).alias(\"is_weekend\")\n",
    "    ])\n",
    "    \n",
    "    # Time of day categories\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"hour\").is_between(0, 5)).then(pl.lit(\"night\"))\n",
    "        .when(pl.col(\"hour\").is_between(6, 11)).then(pl.lit(\"morning\"))\n",
    "        .when(pl.col(\"hour\").is_between(12, 17)).then(pl.lit(\"afternoon\"))\n",
    "        .otherwise(pl.lit(\"evening\"))\n",
    "        .alias(\"time_of_day\")\n",
    "    ])\n",
    "    \n",
    "    # Rush hour flag (7-9 AM or 4-6 PM on weekdays)\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"day_of_week\") < 5) & \n",
    "         ((pl.col(\"hour\").is_between(7, 8)) | \n",
    "          (pl.col(\"hour\").is_between(16, 18)))).cast(pl.Int8).alias(\"is_rush_hour\")\n",
    "    ])\n",
    "    \n",
    "    # Season\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"month_num\").is_between(3, 5)).then(pl.lit(\"spring\"))\n",
    "        .when(pl.col(\"month_num\").is_between(6, 8)).then(pl.lit(\"summer\"))\n",
    "        .when(pl.col(\"month_num\").is_between(9, 11)).then(pl.lit(\"fall\"))\n",
    "        .otherwise(pl.lit(\"winter\"))\n",
    "        .alias(\"season\")\n",
    "    ])\n",
    "    \n",
    "    # Business hours flag (9 AM - 5 PM, Monday-Friday)\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"day_of_week\") < 5) & \n",
    "         (pl.col(\"hour\").is_between(9, 16))).cast(pl.Int8).alias(\"is_business_hours\")\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======================================================================\n",
    "# 2. SPATIAL FEATURES\n",
    "# ======================================================================\n",
    "\n",
    "def create_spatial_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create spatial features from geographic information\n",
    "    \"\"\"\n",
    "    # Check if latitude and longitude are available\n",
    "    has_lat_lon = (\"latitude\" in df.columns) and (\"longitude\" in df.columns)\n",
    "    \n",
    "    if has_lat_lon:\n",
    "        # Filter out invalid coordinates\n",
    "        df = df.with_columns([\n",
    "            pl.when((pl.col(\"latitude\") == 0) | (pl.col(\"longitude\") == 0))\n",
    "            .then(None)\n",
    "            .otherwise(pl.col(\"latitude\"))\n",
    "            .alias(\"latitude\"),\n",
    "            \n",
    "            pl.when((pl.col(\"latitude\") == 0) | (pl.col(\"longitude\") == 0))\n",
    "            .then(None)\n",
    "            .otherwise(pl.col(\"longitude\"))\n",
    "            .alias(\"longitude\")\n",
    "        ])\n",
    "        \n",
    "        # Manhattan distance from city center (approximate location of City Hall)\n",
    "        city_hall_lat = 40.7128\n",
    "        city_hall_lon = -74.0060\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            (pl.abs(pl.col(\"latitude\") - city_hall_lat) + \n",
    "             pl.abs(pl.col(\"longitude\") - city_hall_lon)).alias(\"manhattan_distance_to_city_hall\")\n",
    "        ])\n",
    "        \n",
    "        # Haversine distance to city center\n",
    "        # Note: For performance with large datasets, consider if this precision is necessary\n",
    "        df = df.with_columns([\n",
    "            (2 * 6371 * pl.arcsin(\n",
    "                pl.sqrt(\n",
    "                    pl.sin((pl.col(\"latitude\") - city_hall_lat) * np.pi / 180 / 2) ** 2 +\n",
    "                    pl.cos(city_hall_lat * np.pi / 180) * \n",
    "                    pl.cos(pl.col(\"latitude\") * np.pi / 180) * \n",
    "                    pl.sin((pl.col(\"longitude\") - city_hall_lon) * np.pi / 180 / 2) ** 2\n",
    "                )\n",
    "            )).alias(\"km_distance_to_city_hall\")\n",
    "        ])\n",
    "    \n",
    "    # Create borough encoding if available\n",
    "    if \"borough\" in df.columns:\n",
    "        # One-hot encode boroughs\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"borough\") == \"MANHATTAN\").cast(pl.Int8).alias(\"is_manhattan\"),\n",
    "            (pl.col(\"borough\") == \"BRONX\").cast(pl.Int8).alias(\"is_bronx\"),\n",
    "            (pl.col(\"borough\") == \"BROOKLYN\").cast(pl.Int8).alias(\"is_brooklyn\"),\n",
    "            (pl.col(\"borough\") == \"QUEENS\").cast(pl.Int8).alias(\"is_queens\"),\n",
    "            (pl.col(\"borough\") == \"STATEN ISLAND\").cast(pl.Int8).alias(\"is_staten_island\")\n",
    "        ])\n",
    "    \n",
    "    # Create zip code features\n",
    "    if \"incident_zip\" in df.columns:\n",
    "        # Extract first digit of zip code (geographic region)\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"incident_zip\").cast(pl.Utf8).str.slice(0, 1).cast(pl.Int16).alias(\"zip_first_digit\")\n",
    "        ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======================================================================\n",
    "# 3. CATEGORICAL ENCODING\n",
    "# ======================================================================\n",
    "\n",
    "def encode_categorical_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode categorical features using various methods\n",
    "    \"\"\"\n",
    "    # Create frequency encodings for high cardinality features\n",
    "    categorical_cols = [\"agency\", \"complaint_type\", \"descriptor\", \"location_type\"]\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            # Frequency encoding\n",
    "            freq_counts = df.select(\n",
    "                pl.col(col).alias(\"category\"),\n",
    "                pl.lit(1).alias(\"count\")\n",
    "            ).group_by(\"category\").sum()\n",
    "            \n",
    "            # Join back to original dataframe\n",
    "            df = df.join(\n",
    "                freq_counts.rename({\"category\": col, \"count\": f\"{col}_frequency\"}),\n",
    "                on=col,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            \n",
    "            # Normalize frequency (as percentage of total)\n",
    "            total = freq_counts[\"count\"].sum()\n",
    "            df = df.with_columns([\n",
    "                (pl.col(f\"{col}_frequency\") / total).alias(f\"{col}_frequency_pct\")\n",
    "            ])\n",
    "    \n",
    "    # One-hot encode open_data_channel_type if it exists and has low cardinality\n",
    "    if \"open_data_channel_type\" in df.columns:\n",
    "        channel_counts = df.group_by(\"open_data_channel_type\").count()\n",
    "        \n",
    "        # If cardinality is reasonable, do one-hot encoding\n",
    "        if len(channel_counts) <= 10:  # Arbitrary threshold\n",
    "            unique_channels = channel_counts[\"open_data_channel_type\"].to_list()\n",
    "            for channel in unique_channels:\n",
    "                if channel and isinstance(channel, str):  # Ensure valid values\n",
    "                    df = df.with_columns([\n",
    "                        (pl.col(\"open_data_channel_type\") == channel).cast(pl.Int8)\n",
    "                        .alias(f\"channel_{channel.lower().replace(' ', '_')}\")\n",
    "                    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======================================================================\n",
    "# 4. INTERACTION FEATURES\n",
    "# ======================================================================\n",
    "\n",
    "def create_interaction_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create interaction features between different variables\n",
    "    \"\"\"\n",
    "    # Create agency-complaint type combination feature\n",
    "    if \"agency\" in df.columns and \"complaint_type\" in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"agency\") + \"_\" + pl.col(\"complaint_type\")).alias(\"agency_complaint_combo\")\n",
    "        ])\n",
    "        \n",
    "        # Get frequency count for this combination\n",
    "        combo_counts = df.select(\n",
    "            pl.col(\"agency_complaint_combo\").alias(\"category\"),\n",
    "            pl.lit(1).alias(\"count\")\n",
    "        ).group_by(\"category\").sum()\n",
    "        \n",
    "        # Join back to original dataframe\n",
    "        df = df.join(\n",
    "            combo_counts.rename({\"category\": \"agency_complaint_combo\", \"count\": \"agency_complaint_frequency\"}),\n",
    "            on=\"agency_complaint_combo\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Temporal-spatial interactions\n",
    "    if \"is_weekend\" in df.columns and \"borough\" in df.columns:\n",
    "        # Create weekend-borough combination\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"is_weekend\").cast(pl.Utf8) + \"_\" + pl.col(\"borough\")).alias(\"weekend_borough_combo\")\n",
    "        ])\n",
    "    \n",
    "    # Time of day and complaint type interaction\n",
    "    if \"time_of_day\" in df.columns and \"complaint_type\" in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"time_of_day\") + \"_\" + pl.col(\"complaint_type\")).alias(\"time_complaint_combo\")\n",
    "        ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ======================================================================\n",
    "# 5. AGGREGATION FEATURES\n",
    "# ======================================================================\n",
    "\n",
    "def create_aggregation_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create statistical aggregation features\n",
    "    Note: These operations can be expensive on large datasets\n",
    "    \"\"\"\n",
    "    # Historical response time average by complaint type\n",
    "    if \"complaint_type\" in df.columns and \"response_time_hours\" in df.columns:\n",
    "        avg_response_by_complaint = df.select([\n",
    "            pl.col(\"complaint_type\"),\n",
    "            pl.col(\"response_time_hours\")\n",
    "        ]).group_by(\"complaint_type\").agg([\n",
    "            pl.col(\"response_time_hours\").mean().alias(\"avg_response_time\"),\n",
    "            pl.col(\"response_time_hours\").std().alias(\"std_response_time\"),\n",
    "            pl.col(\"response_time_hours\").count().alias(\"complaint_count\")\n",
    "        ])\n",
    "        \n",
    "        # Join back to original dataframe\n",
    "        df = df.join(\n",
    "            avg_response_by_complaint,\n",
    "            on=\"complaint_type\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Historical response time by agency\n",
    "    if \"agency\" in df.columns and \"response_time_hours\" in df.columns:\n",
    "        avg_response_by_agency = df.select([\n",
    "            pl.col(\"agency\"),\n",
    "            pl.col(\"response_time_hours\")\n",
    "        ]).group_by(\"agency\").agg([\n",
    "            pl.col(\"response_time_hours\").mean().alias(\"agency_avg_response\"),\n",
    "            pl.col(\"response_time_hours\").count().alias(\"agency_request_count\")\n",
    "        ])\n",
    "        \n",
    "        # Join back to original dataframe\n",
    "        df = df.join(\n",
    "            avg_response_by_agency,\n",
    "            on=\"agency\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Complaints by zip code\n",
    "    if \"incident_zip\" in df.columns:\n",
    "        complaints_by_zip = df.select([\n",
    "            pl.col(\"incident_zip\"),\n",
    "            pl.lit(1).alias(\"count\")\n",
    "        ]).group_by(\"incident_zip\").sum()\n",
    "        \n",
    "        # Join back to original dataframe\n",
    "        df = df.join(\n",
    "            complaints_by_zip.rename({\"count\": \"zip_total_complaints\"}),\n",
    "            on=\"incident_zip\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Response times by day of week\n",
    "    if \"day_of_week\" in df.columns and \"response_time_hours\" in df.columns:\n",
    "        avg_by_day = df.select([\n",
    "            pl.col(\"day_of_week\"),\n",
    "            pl.col(\"response_time_hours\")\n",
    "        ]).group_by(\"day_of_week\").agg([\n",
    "            pl.col(\"response_time_hours\").mean().alias(\"day_avg_response\")\n",
    "        ])\n",
    "        \n",
    "        # Join back to original dataframe\n",
    "        df = df.join(\n",
    "            avg_by_day,\n",
    "            on=\"day_of_week\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
