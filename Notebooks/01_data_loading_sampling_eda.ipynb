{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC 311 Service Request Response Time Prediction\n",
    "# Notebook 1: Data Loading, Sampling & Initial EDA\n",
    "\n",
    "# Import libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import gc  # For garbage collection\n",
    "import pandas as pd  # For compatibility with visualization libs if needed\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data path to sys which is a level above the current directory\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# add the parent directory to sys.path\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data with Polars\n",
    "def load_nyc_311_data(file_path):\n",
    "    \"\"\"\n",
    "    Load NYC 311 service request data using Polars\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the data file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Load data with Polars - faster for large datasets\n",
    "    # We'll use streaming mode for very large files\n",
    "    df = pl.scan_csv(file_path)\n",
    "    \n",
    "    # Convert to LazyFrame and print schema\n",
    "    print(\"Data schema:\")\n",
    "    print(df.schema)\n",
    "    \n",
    "    # Get row count using streaming (won't load all data in memory)\n",
    "    row_count = df.select(pl.count()).collect()[0, 0]\n",
    "    print(f\"Total rows: {row_count:,}\")\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Data loaded in {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Path to the data file - adjust as needed\n",
    "data_file = \"../NYC_311_Data/NYC_311_complete.csv\"\n",
    "\n",
    "# Load data\n",
    "df_lazy = load_nyc_311_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 47)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>:@computed_region_7mpf_4k6g</th><th>:@computed_region_92fq_4b7q</th><th>:@computed_region_efsh_h5xi</th><th>:@computed_region_f5dn_yrer</th><th>:@computed_region_sbqj_enih</th><th>:@computed_region_yeji_bk3q</th><th>address_type</th><th>agency</th><th>agency_name</th><th>bbl</th><th>borough</th><th>bridge_highway_direction</th><th>bridge_highway_name</th><th>bridge_highway_segment</th><th>city</th><th>closed_date</th><th>community_board</th><th>complaint_type</th><th>created_date</th><th>cross_street_1</th><th>cross_street_2</th><th>descriptor</th><th>due_date</th><th>facility_type</th><th>incident_address</th><th>incident_zip</th><th>intersection_street_1</th><th>intersection_street_2</th><th>landmark</th><th>latitude</th><th>location</th><th>location_type</th><th>longitude</th><th>open_data_channel_type</th><th>park_borough</th><th>park_facility_name</th><th>resolution_action_updated_date</th><th>resolution_description</th><th>road_ramp</th><th>status</th><th>street_name</th><th>taxi_company_borough</th><th>taxi_pick_up_location</th><th>unique_key</th><th>vehicle_type</th><th>x_coordinate_state_plane</th><th>y_coordinate_state_plane</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>57</td><td>38</td><td>18182</td><td>36</td><td>57</td><td>2</td><td>&quot;ADDRESS&quot;</td><td>&quot;NYPD&quot;</td><td>&quot;New York City Police Departmen…</td><td>3026997501</td><td>&quot;BROOKLYN&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;BROOKLYN&quot;</td><td>&quot;2025-02-20T09:46:41.000&quot;</td><td>&quot;01 BROOKLYN&quot;</td><td>&quot;Illegal Parking&quot;</td><td>&quot;2025-02-20T09:18:31.000&quot;</td><td>&quot;GRAHAM AVENUE&quot;</td><td>&quot;ECKFORD STREET&quot;</td><td>&quot;Blocked Crosswalk&quot;</td><td>null</td><td>null</td><td>&quot;247 DRIGGS AVENUE&quot;</td><td>11222</td><td>&quot;GRAHAM AVENUE&quot;</td><td>&quot;ECKFORD STREET&quot;</td><td>&quot;DRIGGS AVENUE&quot;</td><td>40.722686</td><td>&quot;{&#x27;latitude&#x27;: &#x27;40.7226864471804…</td><td>&quot;Street/Sidewalk&quot;</td><td>-73.947772</td><td>&quot;MOBILE&quot;</td><td>&quot;BROOKLYN&quot;</td><td>&quot;Unspecified&quot;</td><td>&quot;2025-02-20T09:46:44.000&quot;</td><td>&quot;The Police Department responde…</td><td>null</td><td>&quot;Closed&quot;</td><td>&quot;DRIGGS AVENUE&quot;</td><td>null</td><td>null</td><td>64141859</td><td>null</td><td>998727</td><td>202575</td></tr><tr><td>44</td><td>11</td><td>13509</td><td>17</td><td>44</td><td>2</td><td>&quot;ADDRESS&quot;</td><td>&quot;HPD&quot;</td><td>&quot;Department of Housing Preserva…</td><td>3050260070</td><td>&quot;BROOKLYN&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;BROOKLYN&quot;</td><td>&quot;2025-02-20T19:36:41.000&quot;</td><td>&quot;09 BROOKLYN&quot;</td><td>&quot;HEAT/HOT WATER&quot;</td><td>&quot;2025-02-20T09:18:30.000&quot;</td><td>null</td><td>null</td><td>&quot;ENTIRE BUILDING&quot;</td><td>null</td><td>null</td><td>&quot;552 FLATBUSH AVENUE&quot;</td><td>11225</td><td>null</td><td>null</td><td>null</td><td>40.660528</td><td>&quot;{&#x27;latitude&#x27;: &#x27;40.6605280296451…</td><td>&quot;RESIDENTIAL BUILDING&quot;</td><td>-73.960644</td><td>&quot;ONLINE&quot;</td><td>&quot;BROOKLYN&quot;</td><td>&quot;Unspecified&quot;</td><td>&quot;2025-02-20T00:00:00.000&quot;</td><td>&quot;The Department of Housing Pres…</td><td>null</td><td>&quot;Closed&quot;</td><td>&quot;FLATBUSH AVENUE&quot;</td><td>null</td><td>null</td><td>64140044</td><td>null</td><td>995169</td><td>179927</td></tr><tr><td>28</td><td>12</td><td>11270</td><td>43</td><td>28</td><td>5</td><td>&quot;ADDRESS&quot;</td><td>&quot;HPD&quot;</td><td>&quot;Department of Housing Preserva…</td><td>2041637501</td><td>&quot;BRONX&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;BRONX&quot;</td><td>&quot;2025-02-20T17:56:49.000&quot;</td><td>&quot;10 BRONX&quot;</td><td>&quot;HEAT/HOT WATER&quot;</td><td>&quot;2025-02-20T09:18:27.000&quot;</td><td>null</td><td>null</td><td>&quot;ENTIRE BUILDING&quot;</td><td>null</td><td>null</td><td>&quot;1725 EDISON AVENUE&quot;</td><td>10461</td><td>null</td><td>null</td><td>null</td><td>40.845857</td><td>&quot;{&#x27;latitude&#x27;: &#x27;40.8458565695313…</td><td>&quot;RESIDENTIAL BUILDING&quot;</td><td>-73.832637</td><td>&quot;ONLINE&quot;</td><td>&quot;BRONX&quot;</td><td>&quot;Unspecified&quot;</td><td>&quot;2025-02-20T00:00:00.000&quot;</td><td>&quot;The Department of Housing Pres…</td><td>null</td><td>&quot;Closed&quot;</td><td>&quot;EDISON AVENUE&quot;</td><td>null</td><td>null</td><td>64139849</td><td>null</td><td>1030555</td><td>247490</td></tr><tr><td>22</td><td>39</td><td>13098</td><td>47</td><td>22</td><td>4</td><td>&quot;ADDRESS&quot;</td><td>&quot;NYPD&quot;</td><td>&quot;New York City Police Departmen…</td><td>1021790511</td><td>&quot;MANHATTAN&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;NEW YORK&quot;</td><td>&quot;2025-02-21T06:21:25.000&quot;</td><td>&quot;12 MANHATTAN&quot;</td><td>&quot;Blocked Driveway&quot;</td><td>&quot;2025-02-20T09:18:09.000&quot;</td><td>&quot;WEST&nbsp;&nbsp;190 STREET&quot;</td><td>&quot;CABRINI BOULEVARD&quot;</td><td>&quot;Partial Access&quot;</td><td>null</td><td>null</td><td>&quot;701 FORT WASHINGTON AVENUE&quot;</td><td>10040</td><td>&quot;WEST&nbsp;&nbsp;190 STREET&quot;</td><td>&quot;CABRINI BOULEVARD&quot;</td><td>&quot;FORT WASHINGTON AVENUE&quot;</td><td>40.8578</td><td>&quot;{&#x27;latitude&#x27;: &#x27;40.8578003775818…</td><td>&quot;Street/Sidewalk&quot;</td><td>-73.9351</td><td>&quot;PHONE&quot;</td><td>&quot;MANHATTAN&quot;</td><td>&quot;Unspecified&quot;</td><td>&quot;2025-02-21T06:21:29.000&quot;</td><td>&quot;The Police Department responde…</td><td>null</td><td>&quot;Closed&quot;</td><td>&quot;FORT WASHINGTON AVENUE&quot;</td><td>null</td><td>null</td><td>64141525</td><td>null</td><td>1002203</td><td>251804</td></tr><tr><td>61</td><td>6</td><td>24332</td><td>41</td><td>61</td><td>3</td><td>&quot;ADDRESS&quot;</td><td>&quot;DOT&quot;</td><td>&quot;Department of Transportation&quot;</td><td>4108670050</td><td>&quot;QUEENS&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;HOLLIS&quot;</td><td>&quot;2025-02-20T16:32:04.000&quot;</td><td>&quot;12 QUEENS&quot;</td><td>&quot;Street Condition&quot;</td><td>&quot;2025-02-20T09:18:08.000&quot;</td><td>&quot;100 AVENUE&quot;</td><td>&quot;104 AVENUE&quot;</td><td>&quot;Blocked - Construction&quot;</td><td>null</td><td>null</td><td>&quot;100-35 200 STREET&quot;</td><td>11423</td><td>&quot;100 AVENUE&quot;</td><td>&quot;104 AVENUE&quot;</td><td>&quot;200 STREET&quot;</td><td>40.710279</td><td>&quot;{&#x27;latitude&#x27;: &#x27;40.7102791635360…</td><td>&quot;Street&quot;</td><td>-73.759318</td><td>&quot;ONLINE&quot;</td><td>&quot;QUEENS&quot;</td><td>&quot;Unspecified&quot;</td><td>&quot;2025-02-20T16:32:08.000&quot;</td><td>&quot;The Department of Transportati…</td><td>null</td><td>&quot;Closed&quot;</td><td>&quot;200 STREET&quot;</td><td>null</td><td>null</td><td>64144217</td><td>null</td><td>1050976</td><td>198142</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 47)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ :@compute ┆ :@compute ┆ :@compute ┆ :@compute ┆ … ┆ unique_ke ┆ vehicle_t ┆ x_coordin ┆ y_coordi │\n",
       "│ d_region_ ┆ d_region_ ┆ d_region_ ┆ d_region_ ┆   ┆ y         ┆ ype       ┆ ate_state ┆ nate_sta │\n",
       "│ 7mpf_4k6g ┆ 92fq_4b7q ┆ efsh_h5xi ┆ f5dn_yrer ┆   ┆ ---       ┆ ---       ┆ _plane    ┆ te_plane │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ i64       ┆ str       ┆ ---       ┆ ---      │\n",
       "│ i64       ┆ i64       ┆ i64       ┆ i64       ┆   ┆           ┆           ┆ i64       ┆ i64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 57        ┆ 38        ┆ 18182     ┆ 36        ┆ … ┆ 64141859  ┆ null      ┆ 998727    ┆ 202575   │\n",
       "│ 44        ┆ 11        ┆ 13509     ┆ 17        ┆ … ┆ 64140044  ┆ null      ┆ 995169    ┆ 179927   │\n",
       "│ 28        ┆ 12        ┆ 11270     ┆ 43        ┆ … ┆ 64139849  ┆ null      ┆ 1030555   ┆ 247490   │\n",
       "│ 22        ┆ 39        ┆ 13098     ┆ 47        ┆ … ┆ 64141525  ┆ null      ┆ 1002203   ┆ 251804   │\n",
       "│ 61        ┆ 6         ┆ 24332     ┆ 41        ┆ … ┆ 64144217  ┆ null      ┆ 1050976   ┆ 198142   │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display first few rows to verify data loading\n",
    "print(\"Preview of data:\")\n",
    "df_sample = df_lazy.limit(5).collect()\n",
    "display(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':@computed_region_7mpf_4k6g',\n",
       " ':@computed_region_92fq_4b7q',\n",
       " ':@computed_region_efsh_h5xi',\n",
       " ':@computed_region_f5dn_yrer',\n",
       " ':@computed_region_sbqj_enih',\n",
       " ':@computed_region_yeji_bk3q',\n",
       " 'address_type',\n",
       " 'agency',\n",
       " 'agency_name',\n",
       " 'bbl',\n",
       " 'borough',\n",
       " 'bridge_highway_direction',\n",
       " 'bridge_highway_name',\n",
       " 'bridge_highway_segment',\n",
       " 'city',\n",
       " 'closed_date',\n",
       " 'community_board',\n",
       " 'complaint_type',\n",
       " 'created_date',\n",
       " 'cross_street_1',\n",
       " 'cross_street_2',\n",
       " 'descriptor',\n",
       " 'due_date',\n",
       " 'facility_type',\n",
       " 'incident_address',\n",
       " 'incident_zip',\n",
       " 'intersection_street_1',\n",
       " 'intersection_street_2',\n",
       " 'landmark',\n",
       " 'latitude',\n",
       " 'location',\n",
       " 'location_type',\n",
       " 'longitude',\n",
       " 'open_data_channel_type',\n",
       " 'park_borough',\n",
       " 'park_facility_name',\n",
       " 'resolution_action_updated_date',\n",
       " 'resolution_description',\n",
       " 'road_ramp',\n",
       " 'status',\n",
       " 'street_name',\n",
       " 'taxi_company_borough',\n",
       " 'taxi_pick_up_location',\n",
       " 'unique_key',\n",
       " 'vehicle_type',\n",
       " 'x_coordinate_state_plane',\n",
       " 'y_coordinate_state_plane']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_sample(df_lazy, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample of the data to work with\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_lazy : pl.LazyFrame\n",
    "        Original data in lazy format\n",
    "    sample_size : int\n",
    "        Target sample size\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        Sampled data\n",
    "    \"\"\"\n",
    "    print(f\"Creating stratified sample of approximately {sample_size:,} rows...\")\n",
    "    \n",
    "    # 1. Extract key columns for sampling\n",
    "    # Target variable calculation: Response time (time between Created Date and Closed Date)\n",
    "    sample_df = df_lazy.with_columns([\n",
    "        pl.col(\"Created Date\").str.to_datetime(\"%m/%d/%Y %I:%M:%S %p\").alias(\"created_datetime\"),\n",
    "        pl.col(\"Closed Date\").str.to_datetime(\"%m/%d/%Y %I:%M:%S %p\").alias(\"closed_datetime\")\n",
    "    ]).filter(\n",
    "        # Filter out rows where Closed Date is missing (still open) or Created Date is missing\n",
    "        pl.col(\"closed_datetime\").is_not_null() & \n",
    "        pl.col(\"created_datetime\").is_not_null()\n",
    "    ).with_columns([\n",
    "        # Calculate response time in hours\n",
    "        ((pl.col(\"closed_datetime\") - pl.col(\"created_datetime\")).dt.total_seconds() / 3600).alias(\"response_time_hours\")\n",
    "    ]).filter(\n",
    "        # Filter out negative response times (data errors)\n",
    "        pl.col(\"response_time_hours\") >= 0\n",
    "    )\n",
    "    \n",
    "    # 2. Time-based sampling: ensure we have data from different time periods\n",
    "    # Create a year-month column\n",
    "    sample_df = sample_df.with_columns([\n",
    "        pl.col(\"created_datetime\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"created_datetime\").dt.month().alias(\"month\")\n",
    "    ])\n",
    "    \n",
    "    # 3. Balance by agency and complaint type\n",
    "    # Get unique combinations and sample from each\n",
    "    \n",
    "    # First, let's collect basic info about unique agencies and complaint types\n",
    "    agencies_info = sample_df.select(\n",
    "        pl.col(\"Agency\").alias(\"agency\")\n",
    "    ).unique().collect()\n",
    "    \n",
    "    complaint_types_info = sample_df.select(\n",
    "        pl.col(\"Complaint Type\").alias(\"complaint_type\")\n",
    "    ).unique().collect()\n",
    "    \n",
    "    print(f\"Number of unique agencies: {len(agencies_info)}\")\n",
    "    print(f\"Number of unique complaint types: {len(complaint_types_info)}\")\n",
    "    \n",
    "    # Calculate how many samples per agency to aim for\n",
    "    # We'll use a simple approach: equal allocation but with a minimum\n",
    "    target_per_agency = max(100, sample_size // len(agencies_info))\n",
    "    \n",
    "    # Create samples for each agency\n",
    "    agency_samples = []\n",
    "    \n",
    "    for agency in agencies_info[\"agency\"]:\n",
    "        # Sample rows for this agency\n",
    "        agency_sample = sample_df.filter(\n",
    "            pl.col(\"Agency\") == agency\n",
    "        ).sample(\n",
    "            target_per_agency, with_replacement=False\n",
    "        )\n",
    "        \n",
    "        agency_samples.append(agency_sample)\n",
    "    \n",
    "    # Combine all agency samples\n",
    "    combined_sample = pl.concat(agency_samples)\n",
    "    \n",
    "    # If we have more rows than target, take a random sample\n",
    "    final_sample = combined_sample.sample(\n",
    "        min(sample_size, combined_sample.select(pl.count()).collect()[0, 0]), \n",
    "        with_replacement=False\n",
    "    ).collect()\n",
    "    \n",
    "    print(f\"Final sample size: {len(final_sample):,} rows\")\n",
    "    \n",
    "    return final_sample\n",
    "\n",
    "# Create a stratified sample\n",
    "sample_df = create_stratified_sample(df_lazy)\n",
    "\n",
    "# Display sample information\n",
    "print(\"\\nSample information:\")\n",
    "print(f\"Shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Assess data quality issues like missing values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pl.DataFrame\n",
    "        Data to assess\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Assessing data quality...\")\n",
    "    \n",
    "    # 1. Check missing values\n",
    "    missing_values = df.null_count()\n",
    "    missing_pct = (missing_values / len(df) * 100).round(2)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    missing_summary = pl.DataFrame({\n",
    "        \"Column\": missing_values.to_pandas().index,\n",
    "        \"Missing Count\": missing_values.to_pandas().values,\n",
    "        \"Missing Percentage\": missing_pct.to_pandas().values\n",
    "    }).filter(\n",
    "        pl.col(\"Missing Count\") > 0\n",
    "    ).sort(\n",
    "        \"Missing Percentage\", \n",
    "        descending=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMissing Values Summary:\")\n",
    "    print(missing_summary)\n",
    "    \n",
    "    # 2. Check for duplicates\n",
    "    duplicate_count = len(df) - df.select(\"Unique Key\").unique().collect().shape[0]\n",
    "    print(f\"\\nDuplicate Rows: {duplicate_count} ({duplicate_count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Check data types\n",
    "    print(\"\\nData Types:\")\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        print(f\"{col}: {dtype}\")\n",
    "    \n",
    "    # 4. Check for outliers in numeric columns\n",
    "    print(\"\\nNumeric Column Statistics:\")\n",
    "    numeric_cols = [col for col, dtype in zip(df.columns, df.dtypes) \n",
    "                   if dtype in [pl.Int64, pl.Float64, pl.Int32, pl.Float32]]\n",
    "    \n",
    "    if \"response_time_hours\" in df.columns:\n",
    "        numeric_cols.append(\"response_time_hours\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            stats = df.select([\n",
    "                pl.min(col).alias(\"Min\"),\n",
    "                pl.max(col).alias(\"Max\"),\n",
    "                pl.mean(col).alias(\"Mean\"),\n",
    "                pl.median(col).alias(\"Median\"),\n",
    "                pl.std(col).alias(\"Std\")\n",
    "            ]).collect()\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(stats)\n",
    "\n",
    "# Assess data quality\n",
    "assess_data_quality(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distributions(df):\n",
    "    \"\"\"\n",
    "    Analyze distributions of key features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pl.DataFrame\n",
    "        Data to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"Analyzing distributions of key features...\")\n",
    "    \n",
    "    # Convert to pandas for easier plotting with matplotlib/seaborn\n",
    "    pdf = df.to_pandas()\n",
    "    \n",
    "    # 1. Response Time Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(pdf[\"response_time_hours\"], bins=50, kde=True)\n",
    "    plt.title(\"Distribution of Response Time (Hours)\")\n",
    "    plt.xlabel(\"Response Time (Hours)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.axvline(pdf[\"response_time_hours\"].median(), color='r', linestyle='--', label=f'Median: {pdf[\"response_time_hours\"].median():.2f} hours')\n",
    "    plt.axvline(pdf[\"response_time_hours\"].mean(), color='g', linestyle='--', label=f'Mean: {pdf[\"response_time_hours\"].mean():.2f} hours')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"response_time_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if we need to log transform\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(np.log1p(pdf[\"response_time_hours\"]), bins=50, kde=True)\n",
    "    plt.title(\"Distribution of Log Response Time (Hours)\")\n",
    "    plt.xlabel(\"Log Response Time (Hours)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"log_response_time_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Agency Distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    agency_counts = pdf[\"Agency\"].value_counts()\n",
    "    ax = sns.barplot(x=agency_counts.index, y=agency_counts.values)\n",
    "    plt.title(\"Distribution of Service Requests by Agency\")\n",
    "    plt.xlabel(\"Agency\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"agency_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Complaint Type Distribution (Top 20)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    complaint_counts = pdf[\"Complaint Type\"].value_counts().head(20)\n",
    "    ax = sns.barplot(x=complaint_counts.index, y=complaint_counts.values)\n",
    "    plt.title(\"Top 20 Complaint Types\")\n",
    "    plt.xlabel(\"Complaint Type\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"complaint_type_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Response Time by Agency\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    agency_response = pdf.groupby(\"Agency\")[\"response_time_hours\"].agg([\"mean\", \"median\"]).reset_index()\n",
    "    agency_response = agency_response.sort_values(\"median\")\n",
    "    \n",
    "    sns.barplot(x=\"Agency\", y=\"median\", data=agency_response)\n",
    "    plt.title(\"Median Response Time by Agency\")\n",
    "    plt.xlabel(\"Agency\")\n",
    "    plt.ylabel(\"Median Response Time (Hours)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"median_response_time_by_agency.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Response Time by Complaint Type (Top 20)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    complaint_response = pdf.groupby(\"Complaint Type\")[\"response_time_hours\"].agg([\"mean\", \"median\"]).reset_index()\n",
    "    complaint_response = complaint_response.sort_values(\"median\", ascending=False).head(20)\n",
    "    \n",
    "    sns.barplot(x=\"Complaint Type\", y=\"median\", data=complaint_response)\n",
    "    plt.title(\"Top 20 Complaint Types by Median Response Time\")\n",
    "    plt.xlabel(\"Complaint Type\")\n",
    "    plt.ylabel(\"Median Response Time (Hours)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"median_response_time_by_complaint_type.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Temporal Analysis\n",
    "    pdf[\"created_datetime\"] = pd.to_datetime(pdf[\"created_datetime\"])\n",
    "    pdf[\"hour_of_day\"] = pdf[\"created_datetime\"].dt.hour\n",
    "    pdf[\"day_of_week\"] = pdf[\"created_datetime\"].dt.dayofweek\n",
    "    pdf[\"month\"] = pdf[\"created_datetime\"].dt.month\n",
    "    \n",
    "    # By Hour\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hour_response = pdf.groupby(\"hour_of_day\")[\"response_time_hours\"].median()\n",
    "    sns.lineplot(x=hour_response.index, y=hour_response.values)\n",
    "    plt.title(\"Median Response Time by Hour of Day\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Median Response Time (Hours)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"response_time_by_hour.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # By Day of Week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    day_response = pdf.groupby(\"day_of_week\")[\"response_time_hours\"].median()\n",
    "    sns.barplot(x=[day_names[i] for i in day_response.index], y=day_response.values)\n",
    "    plt.title(\"Median Response Time by Day of Week\")\n",
    "    plt.xlabel(\"Day of Week\")\n",
    "    plt.ylabel(\"Median Response Time (Hours)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"response_time_by_day.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # By Month\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    month_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    month_response = pdf.groupby(\"month\")[\"response_time_hours\"].median()\n",
    "    sns.barplot(x=[month_names[i-1] for i in month_response.index], y=month_response.values)\n",
    "    plt.title(\"Median Response Time by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Median Response Time (Hours)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"response_time_by_month.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze distributions\n",
    "analyze_distributions(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample for future use\n",
    "sample_df.write_csv(\"nyc_311_stratified_sample.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"total_rows\": df_lazy.select(pl.count()).collect()[0, 0],\n",
    "    \"sample_rows\": len(sample_df),\n",
    "    \"median_response_time\": sample_df[\"response_time_hours\"].median(),\n",
    "    \"mean_response_time\": sample_df[\"response_time_hours\"].mean(),\n",
    "    \"agency_count\": len(sample_df[\"Agency\"].unique()),\n",
    "    \"complaint_type_count\": len(sample_df[\"Complaint Type\"].unique())\n",
    "}\n",
    "\n",
    "# Write to a text file\n",
    "with open(\"data_summary.txt\", \"w\") as f:\n",
    "    for key, value in summary_stats.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"Results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
